# 1) Confirm the function landed where we expect
apps/core/lib/core/semantic_input.ex:47:def emit_sense_candidates(%{} = si, token_index, scored, lemma, opts \\ []) do
apps/core/lib/core/semantic_input.ex:105:def get_sense_candidates(%{} = si, idx \\ :all) do

# 2) Call sites: PMTG / pre-LIFG
apps/core/test/core/semantic_input_sense_candidates_test.exs:8:  describe "emit_sense_candidates/5" do
apps/core/test/core/semantic_input_sense_candidates_test.exs:19:        SI.emit_sense_candidates(@si, 0, scored, "alpha",
apps/core/test/core/semantic_input_sense_candidates_test.exs:32:        SI.emit_sense_candidates(@si, 2, [
apps/core/test/core/semantic_input_sense_candidates_test.exs:41:        SI.emit_sense_candidates(si1, 2, [
apps/core/test/core/semantic_input_sense_candidates_test.exs:58:        SI.emit_sense_candidates(@si, 1, [
apps/core/test/core/semantic_input_sense_candidates_test.exs:75:    SI.emit_sense_candidates(@si, 3, [
apps/core/test/core/semantic_input_sense_candidates_test.exs:92:        |> SI.emit_sense_candidates(0, [{"a|n|0", 0.7}], "a0")
apps/core/test/core/semantic_input_sense_candidates_test.exs:93:        |> SI.emit_sense_candidates(1, [{"b|n|0", 0.8}], "b0")
apps/core/lib/core/semantic_input.ex:46:@spec emit_sense_candidates(map(), non_neg_integer(), list(), String.t(), keyword()) :: map()
apps/core/lib/core/semantic_input.ex:47:def emit_sense_candidates(%{} = si, token_index, scored, lemma, opts \\ []) do
apps/brain/lib/brain/lifg.ex:1116:defp emit_sense_candidates(si, token_index, scored, lemma) do

# 3) Ensure Stage1 reads candidates safely (no crashes on empty)
apps/brain/lib/brain/atl.ex:183:# ───────── Sense slate promotion → si.sense_candidates ─────────
apps/brain/lib/brain/atl.ex:196:@spec attach_sense_candidates(map(), map(), keyword()) :: map()
apps/brain/lib/brain/atl.ex:197:def attach_sense_candidates(si, slate, opts \\ []) when is_map(si) and is_map(slate) do
apps/brain/lib/brain/atl.ex:198:  candidates = promote_sense_candidates_from_slate(slate, opts)
apps/brain/lib/brain/atl.ex:199:  Map.put(si, :sense_candidates, candidates)
apps/brain/lib/brain/atl.ex:202:@spec promote_sense_candidates_from_slate(map(), keyword()) ::
apps/brain/lib/brain/atl.ex:204:def promote_sense_candidates_from_slate(%{winners: winners} = slate, opts) do
apps/brain/lib/brain/ptmg.ex:498:    si2 = Map.put(si, :sense_candidates, slate_from_ev)
apps/brain/lib/brain/lifg.ex:10:      `Brain.LIFG.Stage1.run/2` — per-token disambiguation from `si.sense_candidates` with guards and tiny heuristics.
apps/brain/lib/brain/lifg.ex:75:        case Map.get(si, :sense_candidates) do
apps/brain/lib/brain/lifg.ex:762:    Stage 1 — fast per-token disambiguation from si.sense_candidates.
apps/brain/lib/brain/lifg.ex:790:        slate  = case Map.get(si, :sense_candidates, Map.get(si, :candidates_by_token, %{})) do
apps/brain/lib/brain/lifg.ex:1116:defp emit_sense_candidates(si, token_index, scored, lemma) do
apps/brain/lib/brain/lifg.ex:1136:  # Stash on si.sense_candidates[token_index]
apps/brain/lib/brain/lifg.ex:1137:  put_in(si, [:sense_candidates, token_index], candidate_list)
apps/brain/lib/brain/lifg.ex:1173:  2) Attach si.sense_candidates
apps/brain/lib/brain/lifg.ex:1192:      # 2) attach slate → sense_candidates (optional)
apps/brain/lib/brain/lifg.ex:1195:             function_exported?(Brain.ATL, :attach_sense_candidates, 3) do
apps/brain/lib/brain/lifg.ex:1196:          case Brain.ATL.attach_sense_candidates(si, slate,

# 4) Run just the new tests
==> brain
     warning: variable "slate" is unused (if the variable is not meant to be used, prefix it with an underscore)
     │
 204 │ def promote_sense_candidates_from_slate(%{winners: winners} = slate, opts) do
     │                                                               ~~~~~
     │
     └─ (brain 0.1.0) lib/brain/atl.ex:204:63: Brain.ATL.promote_sense_candidates_from_slate/2

     warning: def low_confidence?/2 has multiple clauses and also declares default values. In such cases, the default values should be defined in a header. Instead of:

         def foo(:first_clause, b \\ :default) do ... end
         def foo(:second_clause, b) do ... end

     one should write:

         def foo(a, b \\ :default)
         def foo(:first_clause, b) do ... end
         def foo(:second_clause, b) do ... end

     │
 518 │   def low_confidence?(_choice, _opts), do: true
     │       ~
     │
     └─ lib/brain/lifg.ex:518:7

     warning: clauses with the same name and arity (number of arguments) should be grouped together, "defp letter?/1" was previously defined (lib/brain/lifg.ex:898)
     │
 902 │     defp letter?(_), do: false
     │          ~
     │
     └─ lib/brain/lifg.ex:902:10

      warning: function pick_winner_with_margin/1 is unused
      │
 1109 │ defp pick_winner_with_margin(scored) when is_list(scored) and scored != [] do
      │      ~
      │
      └─ lib/brain/lifg.ex:1109:6: Brain.LIFG.Stage1 (module)

      warning: function margin_threshold/0 is unused
      │
 1104 │ defp margin_threshold do
      │      ~
      │
      └─ lib/brain/lifg.ex:1104:6: Brain.LIFG.Stage1 (module)

      warning: function emit_sense_candidates/4 is unused
      │
 1116 │ defp emit_sense_candidates(si, token_index, scored, lemma) do
      │      ~
      │
      └─ lib/brain/lifg.ex:1116:6: Brain.LIFG.Stage1 (module)

     warning: module attribute @small_k_cutoff was set but never used
     │
 773 │     @small_k_cutoff 4
     │     ~~~~~~~~~~~~~~~~~
     │
     └─ lib/brain/lifg.ex:773: Brain.LIFG.Stage1 (module)

     warning: function lex_fit_from_phrase/2 is unused
     │
 205 │   defp lex_fit_from_phrase(phrase, lemma) do
     │        ~
     │
     └─ lib/brain/lifg.ex:205:8: Brain.LIFG (module)

      warning: Brain.ATL.finalize/2 is undefined or private
      │
 1184 │           case Brain.ATL.finalize(si, opts) do
      │                          ~
      │
      └─ (brain 0.1.0) lib/brain/lifg.ex:1184:26: Brain.LIFG.run/2

     warning: this clause of defp word_char?/1 is never used
     │
 751 │     defp word_char?(nil), do: false
     │          ~
     │
     └─ (brain 0.1.0) lib/brain/lifg.ex:751:10: Brain.LIFG.BoundaryGuard.word_char?/1

     warning: the default value for the last optional argument in safe_exec_telemetry/3 is never used
     │
 566 │   defp safe_exec_telemetry(event, measurements \\ %{}, meta \\ %{}) do
     │        ~
     │
     └─ lib/brain/ptmg.ex:566:8: Brain.PMTG (module)

     warning: default values for the optional arguments in emit_rerun_event/2 are never used
     │
 556 │   defp emit_rerun_event(choices, mode \\ :sync) do
     │        ~
     │
     └─ lib/brain/ptmg.ex:556:8: Brain.PMTG (module)

==> core
     warning: Brain.Hippocampus.encode/1 is undefined (module Brain.Hippocampus is not available or is yet to be defined)
     │
 212 │     ep = Brain.Hippocampus.encode(slate)
     │                            ~
     │
     └─ (core 0.1.0) lib/core.ex:212:28: Core.maybe_encode_hippocampus/1

✔ negcache cleared: /data/data/com.termux/files/home/Symbrella/_build/test/lib/core/priv/negcache
✔ negcache cleared: /data/data/com.termux/files/home/Symbrella/apps/core/priv/negcache
==> core
Running ExUnit with seed: 76642, max_cases: 12

.....
Finished in 0.09 seconds (0.09s async, 0.00s sync)
5 tests, 0 failures

# 5) Project-wide test sweep
==> brain
     warning: variable "slate" is unused (if the variable is not meant to be used, prefix it with an underscore)
     │
 204 │ def promote_sense_candidates_from_slate(%{winners: winners} = slate, opts) do
     │                                                               ~~~~~
     │
     └─ (brain 0.1.0) lib/brain/atl.ex:204:63: Brain.ATL.promote_sense_candidates_from_slate/2

     warning: def low_confidence?/2 has multiple clauses and also declares default values. In such cases, the default values should be defined in a header. Instead of:

         def foo(:first_clause, b \\ :default) do ... end
         def foo(:second_clause, b) do ... end

     one should write:

         def foo(a, b \\ :default)
         def foo(:first_clause, b) do ... end
         def foo(:second_clause, b) do ... end

     │
 518 │   def low_confidence?(_choice, _opts), do: true
     │       ~
     │
     └─ lib/brain/lifg.ex:518:7

     warning: clauses with the same name and arity (number of arguments) should be grouped together, "defp letter?/1" was previously defined (lib/brain/lifg.ex:898)
     │
 902 │     defp letter?(_), do: false
     │          ~
     │
     └─ lib/brain/lifg.ex:902:10

      warning: function pick_winner_with_margin/1 is unused
      │
 1109 │ defp pick_winner_with_margin(scored) when is_list(scored) and scored != [] do
      │      ~
      │
      └─ lib/brain/lifg.ex:1109:6: Brain.LIFG.Stage1 (module)

      warning: function margin_threshold/0 is unused
      │
 1104 │ defp margin_threshold do
      │      ~
      │
      └─ lib/brain/lifg.ex:1104:6: Brain.LIFG.Stage1 (module)

      warning: function emit_sense_candidates/4 is unused
      │
 1116 │ defp emit_sense_candidates(si, token_index, scored, lemma) do
      │      ~
      │
      └─ lib/brain/lifg.ex:1116:6: Brain.LIFG.Stage1 (module)

     warning: module attribute @small_k_cutoff was set but never used
     │
 773 │     @small_k_cutoff 4
     │     ~~~~~~~~~~~~~~~~~
     │
     └─ lib/brain/lifg.ex:773: Brain.LIFG.Stage1 (module)

     warning: function lex_fit_from_phrase/2 is unused
     │
 205 │   defp lex_fit_from_phrase(phrase, lemma) do
     │        ~
     │
     └─ lib/brain/lifg.ex:205:8: Brain.LIFG (module)

      warning: Brain.ATL.finalize/2 is undefined or private
      │
 1184 │           case Brain.ATL.finalize(si, opts) do
      │                          ~
      │
      └─ (brain 0.1.0) lib/brain/lifg.ex:1184:26: Brain.LIFG.run/2

     warning: this clause of defp word_char?/1 is never used
     │
 751 │     defp word_char?(nil), do: false
     │          ~
     │
     └─ (brain 0.1.0) lib/brain/lifg.ex:751:10: Brain.LIFG.BoundaryGuard.word_char?/1

     warning: the default value for the last optional argument in safe_exec_telemetry/3 is never used
     │
 566 │   defp safe_exec_telemetry(event, measurements \\ %{}, meta \\ %{}) do
     │        ~
     │
     └─ lib/brain/ptmg.ex:566:8: Brain.PMTG (module)

     warning: default values for the optional arguments in emit_rerun_event/2 are never used
     │
 556 │   defp emit_rerun_event(choices, mode \\ :sync) do
     │        ~
     │
     └─ lib/brain/ptmg.ex:556:8: Brain.PMTG (module)

==> core
     warning: Brain.Hippocampus.encode/1 is undefined (module Brain.Hippocampus is not available or is yet to be defined)
     │
 212 │     ep = Brain.Hippocampus.encode(slate)
     │                            ~
     │
     └─ (core 0.1.0) lib/core.ex:212:28: Core.maybe_encode_hippocampus/1

✔ negcache cleared: /data/data/com.termux/files/home/Symbrella/_build/test/lib/core/priv/negcache
✔ negcache cleared: /data/data/com.termux/files/home/Symbrella/apps/core/priv/negcache
==> lexicon
Running ExUnit with seed: 904668, max_cases: 12

    warning: Lexicon.hello/0 is undefined or private
    │
  6 │     assert Lexicon.hello() == :world
    │                    ~
    │
    └─ test/lexicon_test.exs:6:20: LexiconTest."test greets the world"/1



  1) test greets the world (LexiconTest)
     apps/lexicon/test/lexicon_test.exs:5
     ** (UndefinedFunctionError) function Lexicon.hello/0 is undefined or private
     code: assert Lexicon.hello() == :world
     stacktrace:
       (lexicon 0.1.0) Lexicon.hello()
       test/lexicon_test.exs:6: (test)


Finished in 0.05 seconds (0.00s async, 0.05s sync)
1 test, 1 failure
==> db
Running ExUnit with seed: 904668, max_cases: 12

    warning: Application.get_env/3 is discouraged in the module body, use Application.compile_env/3 instead
    │
  3 │   @dim Application.get_env(:db, :embedding_dim, 1536)
    │                    ~
    │
    └─ test/db/episodes_test.exs:3:20: Db.TestEmbedder (module)

......
Finished in 0.2 seconds (0.1s async, 0.1s sync)
6 tests, 0 failures
==> brain
Running ExUnit with seed: 904668, max_cases: 12

.    warning: ExUnit.Case.register_test/4 is deprecated. Use register_test/6 instead
    │
 84 │   property "softmax normalization yields per-group sums ≈ 1 and score range [0,1]" do
    │   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    │
    └─ test/brain/brain_lifg_property_test.exs:84: BrainLIFGPropertyTest (module)

     warning: ExUnit.Case.register_test/4 is deprecated. Use register_test/6 instead
     │
 108 │   property "boosts target winners; inhibitions exclude winners and cover the rest" do
     │   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     │
     └─ test/brain/brain_lifg_property_test.exs:108: BrainLIFGPropertyTest (module)

....    warning: variable "out" is unused (if the variable is not meant to be used, prefix it with an underscore)
    │
 50 │     out =
    │     ~~~
    │
    └─ test/brain/brain_lifg_integration_test.exs:50:5: BrainLIFGIntegrationTest."test winners stabilize in Top-K; losers drift with decay + inhibitions"/1

***    warning: default values for the optional arguments in lifg_event/3 are never used
    │
  8 │   defp lifg_event(cands, ctx, lifg_opts \\ []) do
    │        ~
    │
    └─ test/brain/brain_lifg_integration_test.exs:8:8: BrainLIFGIntegrationTest (module)

.    warning: redefining module Db.Lexicon (current version loaded from /data/data/com.termux/files/home/Symbrella/_build/test/lib/db/ebin/Elixir.Db.Lexicon.beam)
    │
  1 │ defmodule Db.Lexicon do
    │ ~~~~~~~~~~~~~~~~~~~~~~~
    │
    └─ test/brain/pmtg_rerun_test.exs:1: Db.Lexicon (module)

.    warning: default values for the optional arguments in approx/3 are never used
    │
  8 │   defp approx(a, b, eps \\ 1.0e-6), do: abs(a - b) <= eps
    │        ~
    │
    └─ test/brain/brain_lifg_property_test.exs:8:8: BrainLIFGPropertyTest (module)

...............warning: Range.new/2 and first..last default to a step of -1 when last < first. Use Range.new(first, last, -1) or first..last//-1, or pass 1 if that was your intention
  (stream_data 0.6.0) lib/stream_data.ex:1723: anonymous fn/3 in StreamData.power_of_two_with_zero/1
  (stream_data 0.6.0) lib/stream_data.ex:203: StreamData.call/3
  (stream_data 0.6.0) lib/stream_data.ex:283: anonymous fn/4 in StreamData.float_in_0_to_1/1
  (stream_data 0.6.0) lib/stream_data.ex:203: StreamData.call/3
  (stream_data 0.6.0) lib/stream_data.ex:283: anonymous fn/4 in StreamData.float_with_bounds/2
  (stream_data 0.6.0) lib/stream_data.ex:203: anonymous fn/3 in StreamData.bind_filter/5
  (stream_data 0.6.0) lib/stream_data/lazy_tree.ex:36: StreamData.LazyTree.map/2
  (stream_data 0.6.0) lib/stream_data.ex:393: StreamData.bind_filter/5
  (stream_data 0.6.0) lib/stream_data.ex:360: anonymous fn/5 in StreamData.bind_filter/3
  (stream_data 0.6.0) lib/stream_data.ex:203: StreamData.call/3
  (stream_data 0.6.0) lib/stream_data.ex:203: StreamData.bind_filter/5
  (stream_data 0.6.0) lib/stream_data.ex:360: anonymous fn/5 in StreamData.bind_filter/3
  (stream_data 0.6.0) lib/stream_data.ex:203: anonymous fn/3 in StreamData.bind_filter/5
  (stream_data 0.6.0) lib/stream_data/lazy_tree.ex:36: StreamData.LazyTree.map/2
  (stream_data 0.6.0) lib/stream_data.ex:393: StreamData.bind_filter/5
  (stream_data 0.6.0) lib/stream_data.ex:360: anonymous fn/5 in StreamData.bind_filter/3
  (stream_data 0.6.0) lib/stream_data.ex:203: anonymous fn/3 in StreamData.bind_filter/5

...
Finished in 2.3 seconds (2.3s async, 0.01s sync)
2 properties, 26 tests, 0 failures, 3 skipped
==> core
Running ExUnit with seed: 904668, max_cases: 12

........warning: Range.new/2 and first..last default to a step of -1 when last < first. Use Range.new(first, last, -1) or first..last//-1, or pass 1 if that was your intention
  (core 0.1.0) lib/core/mwe_injector.ex:29: anonymous fn/6 in Core.MWE.Injector.inject/2
  (elixir 1.18.4) lib/enum.ex:4507: Enum.reduce_range/5
  (core 0.1.0) lib/core/mwe_injector.ex:22: Core.MWE.Injector.inject/2
  test/core/mwe_injector_test.exs:41: Core.MWE.InjectorTest."test injects overlapping MWEs when present in repo"/1
  (ex_unit 1.18.4) lib/ex_unit/runner.ex:511: ExUnit.Runner.exec_test/2
  (stdlib 7.0.2) timer.erl:599: :timer.tc/2
  (ex_unit 1.18.4) lib/ex_unit/runner.ex:433: anonymous fn/6 in ExUnit.Runner.spawn_test_monitor/4

....

  1) test merge preserves stronger source precedence (Core.RuntimeBindTest)
     apps/core/test/core/runtime_bind_test.exs:105
     Assertion with == failed
     code:  assert only.activation_snapshot == 0.3
     left:  0.2
     right: 0.3
     stacktrace:
       test/core/runtime_bind_test.exs:130: (test)

.......

  2) test falls back to base defaults when not configured (Core.TokenizerDefaultsTest)
     apps/core/test/core/tokenizer_defaults_test.exs:18
     Assertion with == failed
     code:  assert Input.tokenizer_defaults()[:mode] == :words
     left:  nil
     right: :words
     stacktrace:
       test/core/tokenizer_defaults_test.exs:20: (test)

....

  3) test no char-grams + boundary-only (with allowed MWE) (Core.InvariantsTest)
     apps/core/test/core/invariants_test.exs:7
     ** (ArgumentError) char-gram token leaked: %{index: 3, phrase: "ck t"}
     code: assert :ok == Invariants.assert_no_chargrams!(clean)
     stacktrace:
       (core 0.1.0) lib/core/invariants.ex:18: Core.Invariants.assert_no_chargrams!/1
       test/core/invariants_test.exs:25: (test)

.*

  4) test idempotence: repeated calls produce the same token signature (Core.ResolveInputTest)
     apps/core/test/core/resolve_input_test.exs:159
     ** (DBConnection.OwnershipError) cannot find ownership process for #PID<0.707.0>.

     When using ownership, you must manage connections in one
     of the four ways:

     * By explicitly checking out a connection
     * By explicitly allowing a spawned process
     * By running the pool in shared mode
     * By using :caller option with allowed process

     The first two options require every new process to explicitly
     check a connection out or be allowed by calling checkout or
     allow respectively.

     The third option requires a {:shared, pid} mode to be set.
     If using shared mode in tests, make sure your tests are not
     async.

     The fourth option requires [caller: pid] to be used when
     checking out a connection from the pool. The caller process
     should already be allowed on a connection.

     If you are reading this error, it means you have not done one
     of the steps above or that the owner process has crashed.

     See Ecto.Adapters.SQL.Sandbox docs for more information.
     code: a_si = resolve(sentence, phrase_repo: PhraseRepoFake)
     stacktrace:
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:1095: Ecto.Adapters.SQL.raise_sql_call_error/1
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:996: Ecto.Adapters.SQL.execute/6
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:241: Ecto.Repo.Queryable.execute/4
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:19: Ecto.Repo.Queryable.all/3
       (db 0.1.0) lib/db.ex:50: Db.ltm/2
       (core 0.1.0) lib/core.ex:228: Core.ltm_stage/2
       (core 0.1.0) lib/core.ex:35: Core.resolve_input/2
       test/core/resolve_input_test.exs:161: (test)



  5) test multiword detection: tri-grams produce mw: true; phrase_repo opt tolerated (Core.ResolveInputTest)
     apps/core/test/core/resolve_input_test.exs:141
     ** (DBConnection.OwnershipError) cannot find ownership process for #PID<0.708.0>.

     When using ownership, you must manage connections in one
     of the four ways:

     * By explicitly checking out a connection
     * By explicitly allowing a spawned process
     * By running the pool in shared mode
     * By using :caller option with allowed process

     The first two options require every new process to explicitly
     check a connection out or be allowed by calling checkout or
     allow respectively.

     The third option requires a {:shared, pid} mode to be set.
     If using shared mode in tests, make sure your tests are not
     async.

     The fourth option requires [caller: pid] to be used when
     checking out a connection from the pool. The caller process
     should already be allowed on a connection.

     If you are reading this error, it means you have not done one
     of the steps above or that the owner process has crashed.

     See Ecto.Adapters.SQL.Sandbox docs for more information.
     code: si = resolve("Kick the bucket today", phrase_repo: PhraseRepoFake)
     stacktrace:
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:1095: Ecto.Adapters.SQL.raise_sql_call_error/1
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:996: Ecto.Adapters.SQL.execute/6
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:241: Ecto.Repo.Queryable.execute/4
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:19: Ecto.Repo.Queryable.all/3
       (db 0.1.0) lib/db.ex:50: Db.ltm/2
       (core 0.1.0) lib/core.ex:228: Core.ltm_stage/2
       (core 0.1.0) lib/core.ex:35: Core.resolve_input/2
       test/core/resolve_input_test.exs:142: (test)



  6) test does not crash on emoji, punctuation, or extra whitespace (Core.ResolveInputTest)
     apps/core/test/core/resolve_input_test.exs:169
     ** (DBConnection.OwnershipError) cannot find ownership process for #PID<0.709.0>.

     When using ownership, you must manage connections in one
     of the four ways:

     * By explicitly checking out a connection
     * By explicitly allowing a spawned process
     * By running the pool in shared mode
     * By using :caller option with allowed process

     The first two options require every new process to explicitly
     check a connection out or be allowed by calling checkout or
     allow respectively.

     The third option requires a {:shared, pid} mode to be set.
     If using shared mode in tests, make sure your tests are not
     async.

     The fourth option requires [caller: pid] to be used when
     checking out a connection from the pool. The caller process
     should already be allowed on a connection.

     If you are reading this error, it means you have not done one
     of the steps above or that the owner process has crashed.

     See Ecto.Adapters.SQL.Sandbox docs for more information.
     code: si = resolve(sentence)
     stacktrace:
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:1095: Ecto.Adapters.SQL.raise_sql_call_error/1
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:996: Ecto.Adapters.SQL.execute/6
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:241: Ecto.Repo.Queryable.execute/4
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:19: Ecto.Repo.Queryable.all/3
       (db 0.1.0) lib/db.ex:50: Db.ltm/2
       (core 0.1.0) lib/core.ex:228: Core.ltm_stage/2
       (core 0.1.0) lib/core.ex:35: Core.resolve_input/2
       test/core/resolve_input_test.exs:171: (test)



  7) test span substring roughly matches token.phrase (supports word-index spans) (Core.ResolveInputTest)
     apps/core/test/core/resolve_input_test.exs:117
     ** (DBConnection.OwnershipError) cannot find ownership process for #PID<0.710.0>.

     When using ownership, you must manage connections in one
     of the four ways:

     * By explicitly checking out a connection
     * By explicitly allowing a spawned process
     * By running the pool in shared mode
     * By using :caller option with allowed process

     The first two options require every new process to explicitly
     check a connection out or be allowed by calling checkout or
     allow respectively.

     The third option requires a {:shared, pid} mode to be set.
     If using shared mode in tests, make sure your tests are not
     async.

     The fourth option requires [caller: pid] to be used when
     checking out a connection from the pool. The caller process
     should already be allowed on a connection.

     If you are reading this error, it means you have not done one
     of the steps above or that the owner process has crashed.

     See Ecto.Adapters.SQL.Sandbox docs for more information.
     code: si = resolve(input, phrase_repo: PhraseRepoFake)
     stacktrace:
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:1095: Ecto.Adapters.SQL.raise_sql_call_error/1
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:996: Ecto.Adapters.SQL.execute/6
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:241: Ecto.Repo.Queryable.execute/4
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:19: Ecto.Repo.Queryable.all/3
       (db 0.1.0) lib/db.ex:50: Db.ltm/2
       (core 0.1.0) lib/core.ex:228: Core.ltm_stage/2
       (core 0.1.0) lib/core.ex:35: Core.resolve_input/2
       test/core/resolve_input_test.exs:119: (test)



  8) test tokens (if present) expose phrase + span shape and are sorted by start (Core.ResolveInputTest)
     apps/core/test/core/resolve_input_test.exs:102
     ** (DBConnection.OwnershipError) cannot find ownership process for #PID<0.711.0>.

     When using ownership, you must manage connections in one
     of the four ways:

     * By explicitly checking out a connection
     * By explicitly allowing a spawned process
     * By running the pool in shared mode
     * By using :caller option with allowed process

     The first two options require every new process to explicitly
     check a connection out or be allowed by calling checkout or
     allow respectively.

     The third option requires a {:shared, pid} mode to be set.
     If using shared mode in tests, make sure your tests are not
     async.

     The fourth option requires [caller: pid] to be used when
     checking out a connection from the pool. The caller process
     should already be allowed on a connection.

     If you are reading this error, it means you have not done one
     of the steps above or that the owner process has crashed.

     See Ecto.Adapters.SQL.Sandbox docs for more information.
     code: si = resolve("Hello brave new world")
     stacktrace:
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:1095: Ecto.Adapters.SQL.raise_sql_call_error/1
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:996: Ecto.Adapters.SQL.execute/6
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:241: Ecto.Repo.Queryable.execute/4
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:19: Ecto.Repo.Queryable.all/3
       (db 0.1.0) lib/db.ex:50: Db.ltm/2
       (core 0.1.0) lib/core.ex:228: Core.ltm_stage/2
       (core 0.1.0) lib/core.ex:35: Core.resolve_input/2
       test/core/resolve_input_test.exs:103: (test)



  9) test returns a SemanticInput struct and echoes the sentence/source (Core.ResolveInputTest)
     apps/core/test/core/resolve_input_test.exs:94
     ** (DBConnection.OwnershipError) cannot find ownership process for #PID<0.712.0>.

     When using ownership, you must manage connections in one
     of the four ways:

     * By explicitly checking out a connection
     * By explicitly allowing a spawned process
     * By running the pool in shared mode
     * By using :caller option with allowed process

     The first two options require every new process to explicitly
     check a connection out or be allowed by calling checkout or
     allow respectively.

     The third option requires a {:shared, pid} mode to be set.
     If using shared mode in tests, make sure your tests are not
     async.

     The fourth option requires [caller: pid] to be used when
     checking out a connection from the pool. The caller process
     should already be allowed on a connection.

     If you are reading this error, it means you have not done one
     of the steps above or that the owner process has crashed.

     See Ecto.Adapters.SQL.Sandbox docs for more information.
     code: si = resolve("Hello world")
     stacktrace:
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:1095: Ecto.Adapters.SQL.raise_sql_call_error/1
       (ecto_sql 3.13.2) lib/ecto/adapters/sql.ex:996: Ecto.Adapters.SQL.execute/6
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:241: Ecto.Repo.Queryable.execute/4
       (ecto 3.13.3) lib/ecto/repo/queryable.ex:19: Ecto.Repo.Queryable.all/3
       (db 0.1.0) lib/db.ex:50: Db.ltm/2
       (core 0.1.0) lib/core.ex:228: Core.ltm_stage/2
       (core 0.1.0) lib/core.ex:35: Core.resolve_input/2
       test/core/resolve_input_test.exs:95: (test)


Finished in 0.3 seconds (0.3s async, 0.00s sync)
34 tests, 9 failures, 1 skipped
==> symbrella
There are no tests to run
==> symbrella_web
Running ExUnit with seed: 904668, max_cases: 12

......
Finished in 0.2 seconds (0.2s async, 0.00s sync)
6 tests, 0 failures

# 6) Static style pass
** (Mix) mix format failed due to --check-formatted.
The following files are not formatted:

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/lib/brain/lifg.ex
[0m
           |
  80   80  |      # 3) Score + normalize per token group, pick winners (+ minimal reanalysis)
  81   81  |      weights = resolved_weights(opts)
       82 +|
  82   83  |      {choices, boosts, inhibitions} =
  83   84  |        disambiguate_groups(tokens, candidates, ctx, ctxn, weights, opts)
           |
 147  148  |    cells_normed =
 148  149  |      Enum.map(cells, fn m ->
 149      -|        word  = getf(m, :word) || ""
      150 +|        word = getf(m, :word) || ""
 150  151  |        lemma = getf(m, :lemma) || word
 151      -|        nrm   = getf(m, :norm)  || word
      152 +|        nrm = getf(m, :norm) || word
 152  153  |
 153  154  |        %{
 154      -|          id:           getf(m, :id),
 155      -|          pos:          getf(m, :pos),
 156      -|          lex_fit:      getf(m, :lex_fit),
 157      -|          rel_prior:    getf(m, :rel_prior),
 158      -|          intent_bias:  getf(m, :intent_bias),
 159      -|          activation:   getf(m, :activation),
 160      -|          embedding:    getf(m, :embedding),
 161      -|          definition:   getf(m, :definition),
 162      -|          __norm__:     norm_phrase(nrm),
 163      -|          __lemma__:    lemma
      155 +|          id: getf(m, :id),
      156 +|          pos: getf(m, :pos),
      157 +|          lex_fit: getf(m, :lex_fit),
      158 +|          rel_prior: getf(m, :rel_prior),
      159 +|          intent_bias: getf(m, :intent_bias),
      160 +|          activation: getf(m, :activation),
      161 +|          embedding: getf(m, :embedding),
      162 +|          definition: getf(m, :definition),
      163 +|          __norm__: norm_phrase(nrm),
      164 +|          __lemma__: lemma
 164  165  |        }
 165  166  |      end)
           |
 186  187  |      |> Enum.map(fn c ->
 187  188  |        %{
 188      -|          id:          c.id,
      189 +|          id: c.id,
 189  190  |          token_index: idx,
 190      -|          lemma:       c.__lemma__ || "",
 191      -|          pos:         c.pos,
 192      -|          lex_fit:     c.lex_fit || 0.6,
 193      -|          rel_prior:   c.rel_prior || 0.5,
 194      -|          intent_bias: (c.intent_bias || 0.0) +
 195      -|                         intent_bias_from(si, idx, %{pos: c.pos, definition: c.definition}),
 196      -|          activation:  c.activation || 0.0,
 197      -|          embedding:   c.embedding
      191 +|          lemma: c.__lemma__ || "",
      192 +|          pos: c.pos,
      193 +|          lex_fit: c.lex_fit || 0.6,
      194 +|          rel_prior: c.rel_prior || 0.5,
      195 +|          intent_bias:
      196 +|            (c.intent_bias || 0.0) +
      197 +|              intent_bias_from(si, idx, %{pos: c.pos, definition: c.definition}),
      198 +|          activation: c.activation || 0.0,
      199 +|          embedding: c.embedding
 198  200  |        }
 199  201  |      end)
           |
 206  208  |    p = String.downcase(to_b(phrase))
 207  209  |    l = String.downcase(to_b(lemma))
      210 +|
 208  211  |    cond do
 209  212  |      p == "" or l == "" -> 0.60
 210      -|      p == l             -> 1.00
      213 +|      p == l -> 1.00
 211  214  |      String.contains?(p, l) -> 0.85
 212  215  |      String.contains?(l, p) -> 0.75
 213      -|      true                  -> 0.60
      216 +|      true -> 0.60
 214  217  |    end
 215  218  |  end
           |
 217  220  |  defp intent_bias_from(si, idx, feats_or_map) do
 218  221  |    tokens = Map.get(si, :tokens, [])
 219      -|    cur    = Enum.at(tokens, idx) || %{}
 220      -|    prev   = if idx > 0, do: Enum.at(tokens, idx - 1), else: %{}
      222 +|    cur = Enum.at(tokens, idx) || %{}
      223 +|    prev = if idx > 0, do: Enum.at(tokens, idx - 1), else: %{}
 221  224  |
 222  225  |    phrase = (cur[:phrase] || cur["phrase"] || "") |> to_b()
 223  226  |    prev_w = (prev[:phrase] || prev["phrase"] || "") |> to_b() |> String.downcase()
 224  227  |
 225      -|    pos  =
 226      -|      (getf(feats_or_map, :pos) || getf(feats_or_map, "pos") || "")
 227      -|      |> to_b() |> String.downcase()
      228 +|    pos =
      229 +|      (getf(feats_or_map, :pos) || getf(feats_or_map, "pos") || "") |> to_b() |> String.downcase()
 228  230  |
 229  231  |    defn =
 230  232  |      (getf(feats_or_map, :definition) || getf(feats_or_map, "definition") || "")
 231      -|      |> to_b() |> String.downcase()
      233 +|      |> to_b()
      234 +|      |> String.downcase()
 232  235  |
 233  236  |    at_start? = match?(%{span: {0, _}}, cur) or (cur[:index] || cur["index"] || 0) == 0
           |
 236  239  |    # Salutation: "hello/hi/hey …" → favor interjection at start
 237  240  |    bias =
 238      -|      if at_start? and Regex.match?(~r/^(hello|hi|hey)\b/i, phrase) and String.contains?(pos, "interjection"),
 239      -|        do: bias + 0.20, else: bias
      241 +|      if at_start? and Regex.match?(~r/^(hello|hi|hey)\b/i, phrase) and
      242 +|           String.contains?(pos, "interjection"),
      243 +|         do: bias + 0.20,
      244 +|         else: bias
 240  245  |
 241  246  |    # Definitions gated by "(with "the")" → slightly penalize if not preceded by "the"
 242  247  |    bias =
 243      -|      if String.contains?(defn, ~s|(with "the")|) and prev_w != "the",
 244      -|        do: bias - 0.15, else: bias
      248 +|      if String.contains?(defn, ~s|(with "the")|) and prev_w != "the", do: bias - 0.15, else: bias
 245  249  |
 246  250  |    bias
           |
 252  256  |    groups = Enum.group_by(candidates, & &1.token_index)
 253  257  |
 254      -|    scores_mode      = Keyword.get(opts, :scores, :all)
      258 +|    scores_mode = Keyword.get(opts, :scores, :all)
 255  259  |    margin_threshold = Keyword.get(opts, :margin_threshold, 0.12)
 256      -|    fit?             = Keyword.get(opts, :fit?, fn _cand, _si -> true end)
 257      -|    prime_opts       = Keyword.get(opts, :prime, [])
      260 +|    fit? = Keyword.get(opts, :fit?, fn _cand, _si -> true end)
      261 +|    prime_opts = Keyword.get(opts, :prime, [])
 258  262  |
 259  263  |    {choices, losers} =
           |
 284  288  |            {winner_c, winner_f} =
 285  289  |              case best_c do
 286      -|                nil -> {nil, %{score_norm: 0.0, sim: 0.0, score_raw: 0.0}}
 287      -|                bc  ->
      290 +|                nil ->
      291 +|                  {nil, %{score_norm: 0.0, sim: 0.0, score_raw: 0.0}}
      292 +|
      293 +|                bc ->
 288  294  |                  if fit?.(bc, %{}) do
 289  295  |                    {bc, best_f}
           |
 295  301  |            {_ru_c, ru_f} =
 296  302  |              case normed do
 297      -|                [_only]           -> {nil, %{score_norm: 0.0}}
      303 +|                [_only] -> {nil, %{score_norm: 0.0}}
 298  304  |                [_, {c2, f2} | _] -> {c2, f2}
 299      -|                _                 -> {nil, %{score_norm: 0.0}}
      305 +|                _ -> {nil, %{score_norm: 0.0}}
 300  306  |              end
 301  307  |
           |
 318  324  |
 319  325  |            # cache sorted once for :top2 to avoid double sorts
 320      -|            sorted_normed = if scores_mode == :all, do: [], else: Enum.sort_by(normed, fn {_c, f} -> -f.score_norm end)
      326 +|            sorted_normed =
      327 +|              if scores_mode == :all,
      328 +|                do: [],
      329 +|                else: Enum.sort_by(normed, fn {_c, f} -> -f.score_norm end)
 321  330  |
 322  331  |            scores =
           |
 327  336  |                :top2 ->
 328  337  |                  case sorted_normed do
 329      -|                    []                       -> %{}
 330      -|                    [{c1, f1}]               -> %{c1.id => f1.score_norm}
      338 +|                    [] -> %{}
      339 +|                    [{c1, f1}] -> %{c1.id => f1.score_norm}
 331  340  |                    [{c1, f1}, {c2, f2} | _] -> %{c1.id => f1.score_norm, c2.id => f2.score_norm}
 332  341  |                  end
           |
 344  353  |              margin: margin,
 345  354  |              features:
 346      -|                winner_c && %{
 347      -|                  sim: winner_f.sim,
 348      -|                  score_raw: winner_f.score_raw,
 349      -|                  score_norm: winner_f.score_norm,
 350      -|                  pos: winner_c[:pos],
 351      -|                  lex_fit: winner_c[:lex_fit],
 352      -|                  rel_prior: winner_c[:rel_prior],
 353      -|                  intent_bias: winner_c[:intent_bias],
 354      -|                  activation: winner_c[:activation]
 355      -|                } || %{}
      355 +|                (winner_c &&
      356 +|                   %{
      357 +|                     sim: winner_f.sim,
      358 +|                     score_raw: winner_f.score_raw,
      359 +|                     score_norm: winner_f.score_norm,
      360 +|                     pos: winner_c[:pos],
      361 +|                     lex_fit: winner_c[:lex_fit],
      362 +|                     rel_prior: winner_c[:rel_prior],
      363 +|                     intent_bias: winner_c[:intent_bias],
      364 +|                     activation: winner_c[:activation]
      365 +|                   }) || %{}
 356  366  |            }
 357  367  |
           |
 359  369  |              case winner_c do
 360  370  |                nil -> MapSet.new()
 361      -|                wc  -> cands |> Enum.filter(&(&1.id != wc.id)) |> Enum.map(& &1.id) |> MapSet.new()
      371 +|                wc -> cands |> Enum.filter(&(&1.id != wc.id)) |> Enum.map(& &1.id) |> MapSet.new()
 362  372  |              end
 363  373  |
           |
 375  385  |    choices = Enum.sort_by(choices, & &1.token_index)
 376  386  |
 377      -|    boosts      = for ch <- choices, ch.chosen_id, do: {ch.chosen_id, +0.5}
      387 +|    boosts = for ch <- choices, ch.chosen_id, do: {ch.chosen_id, +0.5}
 378  388  |    inhibitions = Enum.map(losers, &{&1, -0.25})
 379  389  |
           |
 394  404  |
 395  405  |        prime =
 396      -|          if w_prime == 0.0, do: 0.0, else: priming_boost_for(to_string(Map.get(cand, :id, "")), prime_opts)
      406 +|          if w_prime == 0.0,
      407 +|            do: 0.0,
      408 +|            else: priming_boost_for(to_string(Map.get(cand, :id, "")), prime_opts)
 397  409  |
 398  410  |        score =
           |
 417  429  |
 418  430  |  defp best_of_normed([]), do: {nil, %{score_norm: 0.0}}
 419      -|  defp best_of_normed(list), do: Enum.max_by(list, fn {_c, f} -> f.score_norm end, fn -> hd(list) end)
 420  431  |
      432 +|  defp best_of_normed(list),
      433 +|    do: Enum.max_by(list, fn {_c, f} -> f.score_norm end, fn -> hd(list) end)
      434 +|
 421  435  |  defp phrase_for(tokens, idx) do
 422  436  |    case Enum.at(tokens, idx) do
           |
 447  461  |  def cosine(nil, _), do: 0.0
 448  462  |  def cosine(_, nil), do: 0.0
      463 +|
 449  464  |  def cosine(a, b) when is_list(a) and is_list(b) and length(a) == length(b) do
 450  465  |    num = dot(a, b)
           |
 452  467  |    if zeroish?(den, den), do: 0.0, else: num / den
 453  468  |  end
      469 +|
 454  470  |  def cosine(_a, _b), do: 0.0
 455  471  |
           |
 461  477  |    if zeroish?(z, z), do: List.duplicate(0.0, length(scores)), else: Enum.map(exps, &(&1 / z))
 462  478  |  end
      479 +|
 463  480  |  def normalize_scores(_), do: []
 464  481  |
 465  482  |  defp cosine_with_ctxnorm(vec, ctx, ctxn) do
 466  483  |    cond do
 467      -|      is_nil(vec) or is_nil(ctx) -> 0.0
 468      -|      zeroish?(ctxn, ctxn) -> 0.0
      484 +|      is_nil(vec) or is_nil(ctx) ->
      485 +|        0.0
      486 +|
      487 +|      zeroish?(ctxn, ctxn) ->
      488 +|        0.0
      489 +|
 469  490  |      true ->
 470  491  |        na = l2(vec)
           |
 474  495  |  end
 475  496  |
 476      -|  defp dot(a, b) when is_list(a) and is_list(b), do: Enum.zip(a, b) |> Enum.reduce(0.0, fn {x, y}, acc -> acc + safe_num(x) * safe_num(y) end)
      497 +|  defp dot(a, b) when is_list(a) and is_list(b),
      498 +|    do: Enum.zip(a, b) |> Enum.reduce(0.0, fn {x, y}, acc -> acc + safe_num(x) * safe_num(y) end)
      499 +|
 477  500  |  defp dot(_a, _b), do: 0.0
 478      -|  defp l2(v) when is_list(v), do: v |> Enum.reduce(0.0, fn x, acc -> acc + safe_num(x) * safe_num(x) end) |> :math.sqrt()
      501 +|
      502 +|  defp l2(v) when is_list(v),
      503 +|    do: v |> Enum.reduce(0.0, fn x, acc -> acc + safe_num(x) * safe_num(x) end) |> :math.sqrt()
      504 +|
 479  505  |  defp l2(_), do: 0.0
 480  506  |  defp safe_num(nil), do: 0.0
           |
 502  528  |    choice |> Map.get(:scores, %{}) |> Map.values() |> Enum.max(fn -> 0.0 end)
 503  529  |  end
      530 +|
 504  531  |  def top1_prob(_), do: 0.0
 505  532  |
           |
 509  536  |  """
 510  537  |  def low_confidence?(choice, opts \\ []) when is_map(choice) do
 511      -|    tau   = Keyword.get(opts, :tau_confident, 0.20)
      538 +|    tau = Keyword.get(opts, :tau_confident, 0.20)
 512  539  |    p_min = Keyword.get(opts, :p_min, 0.65)
 513      -|    m     = Map.get(choice, :margin, 0.0)
 514      -|    p1    = top1_prob(choice)
      540 +|    m = Map.get(choice, :margin, 0.0)
      541 +|    p1 = top1_prob(choice)
 515  542  |    alts? = (choice[:alt_ids] || []) != []
 516      -|    (m < tau) or (p1 < p_min) or alts?
      543 +|    m < tau or p1 < p_min or alts?
 517  544  |  end
      545 +|
 518  546  |  def low_confidence?(_choice, _opts), do: true
 519  547  |
           |
 525  553  |    |> case do
 526  554  |      nil -> nil
 527      -|      ""  -> nil
 528      -|      v   -> to_string(v)
      555 +|      "" -> nil
      556 +|      v -> to_string(v)
 529  557  |    end
 530  558  |  end
      559 +|
 531  560  |  defp sense_lemma(_), do: nil
 532  561  |
 533  562  |  defp filter_candidates_for_token(list, tok) when is_list(list) and is_map(tok) do
 534  563  |    # MWE token if n>1 or explicit :mw flag
 535      -|    mwe? = (Map.get(tok, :n, 1) > 1) or (Map.get(tok, :mw) in [true, "true"])
      564 +|    mwe? = Map.get(tok, :n, 1) > 1 or Map.get(tok, :mw) in [true, "true"]
 536  565  |
 537  566  |    have_lemma? = Enum.any?(list, &(!is_nil(sense_lemma(&1))))
           |
 540  569  |      Enum.filter(list, fn c ->
 541  570  |        case sense_lemma(c) do
 542      -|          nil     -> true            # no lemma info? keep to be safe
 543      -|          lemma   ->
      571 +|          # no lemma info? keep to be safe
      572 +|          nil ->
      573 +|            true
      574 +|
      575 +|          lemma ->
 544  576  |            has_space = String.contains?(lemma, " ")
 545  577  |            if mwe?, do: has_space, else: not has_space
           |
 556  588  |  defp ensure_list(_), do: []
 557  589  |
 558      -|  ##===================== HYGIENE (optional helper) =====================
      590 +|  ## ===================== HYGIENE (optional helper) =====================
 559  591  |  defmodule Hygiene do
 560  592  |    @moduledoc """
           |
 615  647  |          end)
 616  648  |
 617      -|        meas = %{total: length(choices), sanitized: stats.sanitized, needs_rerun: stats.needs_rerun, ts_ms: now_ms}
      649 +|        meas = %{
      650 +|          total: length(choices),
      651 +|          sanitized: stats.sanitized,
      652 +|          needs_rerun: stats.needs_rerun,
      653 +|          ts_ms: now_ms
      654 +|        }
      655 +|
 618  656  |        emit(event, meas, %{min_margin: min_margin, p_min: p_min})
 619  657  |        {:ok, %{si: si, choices: cleaned, audit: meas}}
           |
 628  666  |      pairs =
 629  667  |        map
 630      -|        |> Enum.filter(fn {_k, v} -> is_number(v) and v == v end) # (v==v) filters NaN
      668 +|        # (v==v) filters NaN
      669 +|        |> Enum.filter(fn {_k, v} -> is_number(v) and v == v end)
 631  670  |
 632  671  |      dropped = map_size(map) - length(pairs)
           |
 635  674  |      probs =
 636  675  |        case vals do
 637      -|          [] -> %{}
      676 +|          [] ->
      677 +|            %{}
      678 +|
 638  679  |          _ ->
 639  680  |            m = Enum.max([0.0 | vals])
           |
 663  704  |  end
 664  705  |
 665      -|  ##===================== TOKEN GUARD (compat) ==========================
      706 +|  ## ===================== TOKEN GUARD (compat) ==========================
 666  707  |  defmodule Guard do
 667  708  |    @moduledoc """
           |
 690  731  |
 691  732  |    defp sort_by_span_if_present(list) when is_list(list) do
 692      -|      if Enum.all?(list, &valid_span?/1), do: Enum.sort_by(list, &elem(Map.fetch!(&1, :span), 0)), else: list
      733 +|      if Enum.all?(list, &valid_span?/1),
      734 +|        do: Enum.sort_by(list, &elem(Map.fetch!(&1, :span), 0)),
      735 +|        else: list
 693  736  |    end
 694  737  |
           |
 697  740  |  end
 698  741  |
 699      -|  ##================== TOKEN BOUNDARY GUARD (compat) ====================
      742 +|  ## ================== TOKEN BOUNDARY GUARD (compat) ====================
 700  743  |  defmodule BoundaryGuard do
 701  744  |    @moduledoc """
           |
 713  756  |      |> Enum.filter(fn t ->
 714  757  |        mw? = truthy(Map.get(t, :mw) || Map.get(t, "mw"))
      758 +|
 715  759  |        case {Map.get(t, :span) || Map.get(t, "span"), sentence} do
 716  760  |          {{s, l}, snt} when is_integer(s) and is_integer(l) and l > 0 and is_binary(snt) ->
 717  761  |            boundary_aligned?(snt, s, l) or mw?
 718      -|          _ -> true
      762 +|
      763 +|          _ ->
      764 +|            true
 719  765  |        end
 720  766  |      end)
           |
 728  774  |    defp chargram?(t) when is_map(t),
 729  775  |      do:
 730      -|        (Map.get(t, :chargram) in [true, "true"]) or
 731      -|        (Map.get(t, :kind) in [:chargram, "chargram"]) or
 732      -|        (Map.get(t, :source) in [:chargram, "chargram"])
      776 +|        Map.get(t, :chargram) in [true, "true"] or
      777 +|          Map.get(t, :kind) in [:chargram, "chargram"] or
      778 +|          Map.get(t, :source) in [:chargram, "chargram"]
      779 +|
 733  780  |    defp chargram?(_), do: false
 734  781  |
 735  782  |    defp sort_by_span_if_present(list) when is_list(list) do
 736      -|      if Enum.all?(list, &valid_span?/1), do: Enum.sort_by(list, &elem(Map.fetch!(&1, :span), 0)), else: list
      783 +|      if Enum.all?(list, &valid_span?/1),
      784 +|        do: Enum.sort_by(list, &elem(Map.fetch!(&1, :span), 0)),
      785 +|        else: list
 737  786  |    end
 738  787  |
           |
 747  796  |      left_ok and right_ok
 748  797  |    end
      798 +|
 749  799  |    defp boundary_aligned?(_snt, _s, _l), do: false
 750  800  |
           |
 757  807  |  end
 758  808  |
 759      -|  ##========================= STAGE 1 (new) ============================
      809 +|  ## ========================= STAGE 1 (new) ============================
 760  810  |  defmodule Stage1 do
 761  811  |    @moduledoc """
           |
 776  826  |    @type choice :: %{
 777  827  |            required(:token_index) => non_neg_integer(),
 778      -|            required(:chosen_id)   => String.t(),
 779      -|            required(:alt_ids)     => [String.t()],
 780      -|            required(:margin)      => float(),
 781      -|            optional(:scores)      => map()
      828 +|            required(:chosen_id) => String.t(),
      829 +|            required(:alt_ids) => [String.t()],
      830 +|            required(:margin) => float(),
      831 +|            optional(:scores) => map()
 782  832  |          }
 783  833  |
 784      -|    @spec run(si(), keyword()) :: {:ok, %{si: si(), choices: [choice()], audit: map()}} | {:error, term()}
      834 +|    @spec run(si(), keyword()) ::
      835 +|            {:ok, %{si: si(), choices: [choice()], audit: map()}} | {:error, term()}
 785  836  |    def run(si, opts \\ []) when is_map(si) and is_list(opts) do
 786  837  |      try do
 787  838  |        t0 = System.monotonic_time()
 788  839  |
 789      -|        tokens = Map.get(si, :tokens, [])# |> ensure_list()
 790      -|        slate  = case Map.get(si, :sense_candidates, Map.get(si, :candidates_by_token, %{})) do
 791      -|          %{} = s -> s
 792      -|          _ -> %{}
 793      -|        end
      840 +|        # |> ensure_list()
      841 +|        tokens = Map.get(si, :tokens, [])
 794  842  |
      843 +|        slate =
      844 +|          case Map.get(si, :sense_candidates, Map.get(si, :candidates_by_token, %{})) do
      845 +|            %{} = s -> s
      846 +|            _ -> %{}
      847 +|          end
      848 +|
 795  849  |        {kept_tokens, dropped} = guard_tokens(tokens, si, opts)
 796  850  |
           |
 833  887  |
 834  888  |    # Back-compat shim (si, ctx, opts)
 835      -|    @spec run(si(), map(), keyword()) :: {:ok, %{si: si(), choices: [choice()], audit: map()}} | {:error, term()}
      889 +|    @spec run(si(), map(), keyword()) ::
      890 +|            {:ok, %{si: si(), choices: [choice()], audit: map()}} | {:error, term()}
 836  891  |    def run(si, _ctx, opts), do: run(si, opts)
 837  892  |
           |
 840  895  |    defp guard_tokens(tokens, si, opts) when is_list(tokens) and is_map(si) do
 841  896  |      char_ev = Keyword.get(opts, :chargram_event, [:brain, :lifg, :chargram_violation])
 842      -|      bnd_ev  = Keyword.get(opts, :boundary_event,  [:brain, :lifg, :boundary_drop])
      897 +|      bnd_ev = Keyword.get(opts, :boundary_event, [:brain, :lifg, :boundary_drop])
 843  898  |
 844  899  |      Enum.reduce(tokens, {[], []}, fn tok, {kept, dropped} ->
           |
 858  913  |      |> (fn {k, d} -> {Enum.reverse(k), Enum.reverse(d)} end).()
 859  914  |    end
      915 +|
 860  916  |    defp guard_tokens(_tokens, _si, _opts), do: {[], []}
 861  917  |
 862  918  |    defp is_chargram?(tok) when is_map(tok) do
 863      -|      (Map.get(tok, :source) in [:chargram, "chargram"]) or
 864      -|        (Map.get(tok, :kind)   in [:chargram, "chargram"]) or
 865      -|        (Map.get(tok, :chargram) in [true, "true"])
      919 +|      Map.get(tok, :source) in [:chargram, "chargram"] or
      920 +|        Map.get(tok, :kind) in [:chargram, "chargram"] or
      921 +|        Map.get(tok, :chargram) in [true, "true"]
 866  922  |    end
      923 +|
 867  924  |    defp is_chargram?(_), do: false
 868  925  |
           |
 872  929  |      if mw?, do: true, else: do_boundary_check(tok, si)
 873  930  |    end
      931 +|
 874  932  |    defp boundary_ok?(_tok, _si), do: false
 875  933  |
           |
 879  937  |
 880  938  |      cond do
 881      -|        not is_binary(sent) -> true
 882      -|        not (is_tuple(span) and tuple_size(span) == 2) -> true
      939 +|        not is_binary(sent) ->
      940 +|          true
      941 +|
      942 +|        not (is_tuple(span) and tuple_size(span) == 2) ->
      943 +|          true
      944 +|
 883  945  |        true ->
 884  946  |          {start, len} = span
 885  947  |          start = as_int(start, 0)
 886      -|          len   = as_int(len, 0)
 887      -|          stop  = start + len
      948 +|          len = as_int(len, 0)
      949 +|          stop = start + len
 888  950  |          s_len = byte_size(sent)
 889  951  |
 890      -|          left_ok?  = start <= 0 or not letter?(String.at(sent, start - 1))
      952 +|          left_ok? = start <= 0 or not letter?(String.at(sent, start - 1))
 891  953  |          right_ok? = stop >= s_len or not letter?(String.at(sent, stop))
 892  954  |
           |
 894  956  |      end
 895  957  |    end
      958 +|
 896  959  |    defp do_boundary_check(_tok, _si), do: true
 897  960  |
 898  961  |    defp letter?(nil), do: false
 899  962  |    defp letter?(<<c::utf8>>), do: unicode_letter?(c)
      963 +|
 900  964  |    defp unicode_letter?(cp),
 901  965  |      do: (cp >= ?A and cp <= ?Z) or (cp >= ?a and cp <= ?z) or (cp >= ?À and cp <= 0x024F)
      966 +|
 902  967  |    defp letter?(_), do: false
 903  968  |
           |
 905  970  |
 906  971  |    defp disambiguate_token(%{} = tok, cand_list, opts) when is_list(cand_list) do
 907      -|      idx         = tok.index || 0
 908      -|      thr         = Keyword.get(opts, :margin_threshold, 0.15)
      972 +|      idx = tok.index || 0
      973 +|      thr = Keyword.get(opts, :margin_threshold, 0.15)
 909  974  |      scores_mode = Keyword.get(opts, :scores, :all)
 910      -|      w           = weights(opts)
      975 +|      w = weights(opts)
 911  976  |
 912  977  |      # sanitize input candidates (nil/id-less/dupes) and apply tiny heuristics
           |
 922  987  |      score_cand = fn c when is_map(c) ->
 923  988  |        f = Map.get(c, :features, %{})
      989 +|
 924  990  |        %{
 925  991  |          id: c.id,
 926  992  |          score:
 927      -|            w.lex_fit     * as_float(f[:lex_fit]     || f["lex_fit"]) +
 928      -|            w.rel_prior   * as_float(f[:rel_prior]   || f["rel_prior"]) +
 929      -|            w.activation  * as_float(f[:activation]  || f["activation"]) +
 930      -|            w.intent_bias * as_float(f[:intent_bias] || f["intent_bias"])
      993 +|            w.lex_fit * as_float(f[:lex_fit] || f["lex_fit"]) +
      994 +|              w.rel_prior * as_float(f[:rel_prior] || f["rel_prior"]) +
      995 +|              w.activation * as_float(f[:activation] || f["activation"]) +
      996 +|              w.intent_bias * as_float(f[:intent_bias] || f["intent_bias"])
 931  997  |        }
 932  998  |      end
           |
 968 1034  |      end
 969 1035  |    end
     1036 +|
 970 1037  |    defp disambiguate_token(_tok, _cand_list, _opts), do: nil
 971 1038  |
           |
 973 1040  |    defp prefer_salutation_interjection(list, tok) when is_list(list) and is_map(tok) do
 974 1041  |      phrase = (tok[:phrase] || "") |> to_string()
     1042 +|
 975 1043  |      at_start =
 976 1044  |        case tok[:span] do
           |
 985 1053  |          Enum.filter(list, fn c when is_map(c) ->
 986 1054  |            pos =
 987      -|              (c[:pos] || get_in(c, [:features, :pos]) || "")
 988      -|              |> to_string() |> String.downcase()
     1055 +|              (c[:pos] || get_in(c, [:features, :pos]) || "") |> to_string() |> String.downcase()
 989 1056  |
 990 1057  |            defn =
 991 1058  |              (c[:definition] || get_in(c, [:features, :definition]) || "")
 992      -|              |> to_string() |> String.downcase()
     1059 +|              |> to_string()
     1060 +|              |> String.downcase()
 993 1061  |
 994 1062  |            syns =
           |
1005 1073  |      end
1006 1074  |    end
     1075 +|
1007 1076  |    defp prefer_salutation_interjection(list, _tok), do: list
1008 1077  |
           |
1010 1079  |    defp prefer_pronoun_for_I(list, tok) when is_list(list) and is_map(tok) do
1011 1080  |      phrase = (Map.get(tok, :phrase) || Map.get(tok, "phrase") || "") |> to_string()
     1081 +|
1012 1082  |      if phrase == "I" do
1013      -|        pronounish = Enum.filter(list, fn c when is_map(c) -> pos_pronoun?(c) or id_pronounish?(c) end)
     1083 +|        pronounish =
     1084 +|          Enum.filter(list, fn c when is_map(c) -> pos_pronoun?(c) or id_pronounish?(c) end)
     1085 +|
1014 1086  |        if pronounish == [], do: list, else: pronounish
1015 1087  |      else
           |
1017 1089  |      end
1018 1090  |    end
     1091 +|
1019 1092  |    defp prefer_pronoun_for_I(list, _tok), do: list
1020 1093  |
           |
1032 1105  |      end
1033 1106  |    end
     1107 +|
1034 1108  |    defp pos_pronoun?(_), do: false
1035 1109  |
           |
1038 1112  |      String.contains?(String.downcase(id), "pron")
1039 1113  |    end
     1114 +|
1040 1115  |    defp id_pronounish?(_), do: false
1041 1116  |
1042 1117  |    # Prefer MWE senses for MWE tokens (and vice versa), with safe fallback
1043      -|    defp compat_filter_for_token(list, %{n: n} = tok) when is_list(list) and is_map(tok) and is_integer(n) do
     1118 +|    defp compat_filter_for_token(list, %{n: n} = tok)
     1119 +|         when is_list(list) and is_map(tok) and is_integer(n) do
1044 1120  |      mwe? = n > 1 or (Map.get(tok, :mw) || Map.get(tok, "mw") || false)
1045 1121  |      have_lemma? = Enum.any?(list, &has_readable_lemma?/1)
           |
1048 1124  |        Enum.filter(list, fn c when is_map(c) ->
1049 1125  |          case sense_lemma(c) do
1050      -|            nil -> true  # if unknown, keep
     1126 +|            # if unknown, keep
     1127 +|            nil ->
     1128 +|              true
     1129 +|
1051 1130  |            lemma ->
1052 1131  |              has_space = String.contains?(lemma, " ")
           |
1057 1136  |      if have_lemma? and kept == [], do: list, else: kept
1058 1137  |    end
     1138 +|
1059 1139  |    defp compat_filter_for_token(list, _tok), do: list
1060 1140  |
           |
1068 1148  |      |> case do
1069 1149  |        nil -> nil
1070      -|        ""  -> nil
1071      -|        v   -> to_string(v)
     1150 +|        "" -> nil
     1151 +|        v -> to_string(v)
1072 1152  |      end
1073 1153  |    end
     1154 +|
1074 1155  |    defp sense_lemma(_), do: nil
1075 1156  |
           |
1099 1180  |    end
1100 1181  |
1101      -|# === in apps/brain/lib/brain/lifg_stage1.ex (inside defmodule Brain.LIFG.Stage1) ===
     1182 +|    # === in apps/brain/lib/brain/lifg_stage1.ex (inside defmodule Brain.LIFG.Stage1) ===
1102 1183  |
1103      -|# Runtime-configurable threshold (fallback 0.15)
1104      -|defp margin_threshold do
1105      -|  Application.get_env(:brain, :pmtg_margin_threshold, 0.15)
1106      -|end
     1184 +|    # Runtime-configurable threshold (fallback 0.15)
     1185 +|    defp margin_threshold do
     1186 +|      Application.get_env(:brain, :pmtg_margin_threshold, 0.15)
     1187 +|    end
1107 1188  |
1108      -|# scored :: [%{id: id, score: float, lemma: lemma, token_index: integer} | ...]
1109      -|defp pick_winner_with_margin(scored) when is_list(scored) and scored != [] do
1110      -|  [top | rest] = Enum.sort_by(scored, & &1.score, :desc)
1111      -|  second = List.first(rest)
1112      -|  margin = if second, do: top.score - second.score, else: 1.0
1113      -|  {Map.put(top, :margin, margin), if(second, do: Map.put(second, :margin, margin), else: nil)}
1114      -|end
1115      -|
1116      -|defp emit_sense_candidates(si, token_index, scored, lemma) do
1117      -|  {winner, second} = pick_winner_with_margin(scored)
1118      -|  near =
1119      -|    if second && winner.margin < margin_threshold() do
1120      -|      :telemetry.execute(
1121      -|        [:brain, :lifg, :low_margin],
1122      -|        %{margin: winner.margin},
1123      -|        %{token_index: token_index, winner_id: winner.id, second_id: second.id}
1124      -|      )
1125      -|      [%{second | near: true}]
1126      -|    else
1127      -|      []
     1189 +|    # scored :: [%{id: id, score: float, lemma: lemma, token_index: integer} | ...]
     1190 +|    defp pick_winner_with_margin(scored) when is_list(scored) and scored != [] do
     1191 +|      [top | rest] = Enum.sort_by(scored, & &1.score, :desc)
     1192 +|      second = List.first(rest)
     1193 +|      margin = if second, do: top.score - second.score, else: 1.0
     1194 +|      {Map.put(top, :margin, margin), if(second, do: Map.put(second, :margin, margin), else: nil)}
1128 1195  |    end
1129 1196  |
1130      -|  candidate_list =
1131      -|    [%{winner | lemma: lemma, token_index: token_index, near: false} | near]
1132      -|    |> Enum.map(fn c ->
1133      -|      Map.take(c, [:id, :score, :lemma, :token_index, :margin, :near])
1134      -|    end)
     1197 +|    defp emit_sense_candidates(si, token_index, scored, lemma) do
     1198 +|      {winner, second} = pick_winner_with_margin(scored)
1135 1199  |
1136      -|  # Stash on si.sense_candidates[token_index]
1137      -|  put_in(si, [:sense_candidates, token_index], candidate_list)
1138      -|end
     1200 +|      near =
     1201 +|        if second && winner.margin < margin_threshold() do
     1202 +|          :telemetry.execute(
     1203 +|            [:brain, :lifg, :low_margin],
     1204 +|            %{margin: winner.margin},
     1205 +|            %{token_index: token_index, winner_id: winner.id, second_id: second.id}
     1206 +|          )
1139 1207  |
     1208 +|          [%{second | near: true}]
     1209 +|        else
     1210 +|          []
     1211 +|        end
1140 1212  |
     1213 +|      candidate_list =
     1214 +|        [%{winner | lemma: lemma, token_index: token_index, near: false} | near]
     1215 +|        |> Enum.map(fn c ->
     1216 +|          Map.take(c, [:id, :score, :lemma, :token_index, :margin, :near])
     1217 +|        end)
     1218 +|
     1219 +|      # Stash on si.sense_candidates[token_index]
     1220 +|      put_in(si, [:sense_candidates, token_index], candidate_list)
     1221 +|    end
     1222 +|
1141 1223  |    defp emit(ev, meas, meta) do
1142 1224  |      if Code.ensure_loaded?(:telemetry) and function_exported?(:telemetry, :execute, 3) do
           |
1149 1231  |    defp as_int(nil, d), do: d
1150 1232  |    defp as_int(i, _d) when is_integer(i), do: i
     1233 +|
1151 1234  |    defp as_int(b, d) when is_binary(b) do
1152 1235  |      case Integer.parse(b) do
           |
1155 1238  |      end
1156 1239  |    end
     1240 +|
1157 1241  |    defp as_int(_, d), do: d
1158 1242  |
1159 1243  |    defp as_float(nil), do: 0.0
1160 1244  |    defp as_float(n) when is_number(n), do: n * 1.0
     1245 +|
1161 1246  |    defp as_float(b) when is_binary(b) do
1162 1247  |      case Float.parse(b) do
           |
1165 1250  |      end
1166 1251  |    end
     1252 +|
1167 1253  |    defp as_float(_), do: 0.0
1168 1254  |  end
           |
1176 1262  |  Returns {:ok, %{si, choices, slate}}.
1177 1263  |  """
1178      -|  @spec run(map(), keyword()) :: {:ok, %{si: map(), choices: list(), slate: map()}} | {:error, term()}
     1264 +|  @spec run(map(), keyword()) ::
     1265 +|          {:ok, %{si: map(), choices: list(), slate: map()}} | {:error, term()}
1179 1266  |  def run(si, opts \\ []) when is_map(si) and is_list(opts) do
1180 1267  |    try do
           |
1195 1282  |             function_exported?(Brain.ATL, :attach_sense_candidates, 3) do
1196 1283  |          case Brain.ATL.attach_sense_candidates(si, slate,
1197      -|                   top_k: Keyword.get(opts, :top_k, 3),
1198      -|                   margin_window: Keyword.get(opts, :margin_window, 0.05)
1199      -|                 ) do
     1284 +|                 top_k: Keyword.get(opts, :top_k, 3),
     1285 +|                 margin_window: Keyword.get(opts, :margin_window, 0.05)
     1286 +|               ) do
1200 1287  |            %{} = s -> s
1201 1288  |            _ -> si
           |
1206 1293  |
1207 1294  |      # 3) Stage-1
1208      -|      scores_mode = Keyword.get(opts, :scores, Application.get_env(:brain, :lifg_stage1_scores_mode, :all))
1209      -|      margin_thr  = Keyword.get(opts, :margin_threshold, 0.15)
     1295 +|      scores_mode =
     1296 +|        Keyword.get(opts, :scores, Application.get_env(:brain, :lifg_stage1_scores_mode, :all))
1210 1297  |
     1298 +|      margin_thr = Keyword.get(opts, :margin_threshold, 0.15)
     1299 +|
1211 1300  |      case Stage1.run(si,
1212      -|               weights: Map.get(opts, :weights, %{
1213      -|                 lex_fit: 0.5, rel_prior: 0.25, activation: 0.15, intent_bias: 0.10
     1301 +|             weights:
     1302 +|               Map.get(opts, :weights, %{
     1303 +|                 lex_fit: 0.5,
     1304 +|                 rel_prior: 0.25,
     1305 +|                 activation: 0.15,
     1306 +|                 intent_bias: 0.10
1214 1307  |               }),
1215      -|               scores: scores_mode,
1216      -|               margin_threshold: margin_thr,
1217      -|               chargram_event: [:brain, :lifg, :chargram_violation],
1218      -|               boundary_event: [:brain, :lifg, :boundary_drop]
1219      -|             ) do
     1308 +|             scores: scores_mode,
     1309 +|             margin_threshold: margin_thr,
     1310 +|             chargram_event: [:brain, :lifg, :chargram_violation],
     1311 +|             boundary_event: [:brain, :lifg, :boundary_drop]
     1312 +|           ) do
1220 1313  |        {:ok, %{si: si, choices: choices, audit: _a}} ->
1221 1314  |          # 4) pMTG
1222      -|          pmtg_mode = Keyword.get(opts, :pmtg_mode, Application.get_env(:brain, :pmtg_mode, :boost))
     1315 +|          pmtg_mode =
     1316 +|            Keyword.get(opts, :pmtg_mode, Application.get_env(:brain, :pmtg_mode, :boost))
1223 1317  |
1224 1318  |          final_choices =
           |
1234 1328  |                         mode: :rerun,
1235 1329  |                         rerun_only_if_hits: Keyword.get(opts, :rerun_only_if_hits, true),
1236      -|                         rerun_weights_bump: Keyword.get(opts, :rerun_weights_bump, %{lex_fit: 0.05, rel_prior: 0.05})
     1330 +|                         rerun_weights_bump:
     1331 +|                           Keyword.get(opts, :rerun_weights_bump, %{
     1332 +|                             lex_fit: 0.05,
     1333 +|                             rel_prior: 0.05
     1334 +|                           })
1237 1335  |                       ) do
1238 1336  |                    {:ok, %{choices: merged}} -> merged
           |
1291 1389  |  defp resolved_weights(opts) when is_list(opts) do
1292 1390  |    outer_env = to_map(Application.get_env(:brain, :lifg_weights))
     1391 +|
1293 1392  |    mapped_from_stage1 =
1294 1393  |      case to_map(Application.get_env(:brain, :lifg_stage1_weights)) do
1295 1394  |        %{} = w1 when map_size(w1) > 0 ->
1296 1395  |          %{
1297      -|            lex:  Map.get(w1, :lex_fit,     default_weights().lex),
1298      -|            sim:  default_weights().sim, # stage-1 has no :sim
1299      -|            rel:  Map.get(w1, :rel_prior,   default_weights().rel),
     1396 +|            lex: Map.get(w1, :lex_fit, default_weights().lex),
     1397 +|            # stage-1 has no :sim
     1398 +|            sim: default_weights().sim,
     1399 +|            rel: Map.get(w1, :rel_prior, default_weights().rel),
1300 1400  |            prag: Map.get(w1, :intent_bias, default_weights().prag),
1301      -|            act:  Map.get(w1, :activation,  default_weights().act)
     1401 +|            act: Map.get(w1, :activation, default_weights().act)
1302 1402  |          }
1303      -|        _ -> %{}
     1403 +|
     1404 +|        _ ->
     1405 +|          %{}
1304 1406  |      end
1305 1407  |
1306 1408  |    env_weights = if outer_env == %{}, do: mapped_from_stage1, else: outer_env
1307      -|    opt_w      = to_map(Keyword.get(opts, :weights, %{}))
     1409 +|    opt_w = to_map(Keyword.get(opts, :weights, %{}))
1308 1410  |
1309 1411  |    default_weights()
           |
1313 1415  |end
1314 1416  |
1315      -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/symbrella_web/lib/symbrella_web/live/home_live.ex
[0m
         |
 25  25  |  # A tiny formatter so the UI shows something friendly when SI comes back.
 26  26  |  defp format_si_reply(si) do
 27     -|    tokens  = Map.get(si, :tokens, [])
 28     -|    cells   = Map.get(si, :active_cells, Map.get(si, :cells, []))
     27 +|    tokens = Map.get(si, :tokens, [])
     28 +|    cells = Map.get(si, :active_cells, Map.get(si, :cells, []))
 29  29  |    choices = Map.get(si, :lifg_choices, [])
 30  30  |
         |
 46  46  |      |> Enum.join(", ")
 47  47  |
 48     -|   base =
 49     -|  "Got it. tokens=#{length(tokens)}, cells=#{length(cells)}, lifg=#{length(choices)}" <>
 50     -|    " · src=#{inspect(Map.get(si, :source))}" <>
 51     -|    if token_preview != "", do: " · [#{token_preview}]", else: ""
 52     -| 
     48 +|    base =
     49 +|      "Got it. tokens=#{length(tokens)}, cells=#{length(cells)}, lifg=#{length(choices)}" <>
     50 +|        " · src=#{inspect(Map.get(si, :source))}" <>
     51 +|        if token_preview != "", do: " · [#{token_preview}]", else: ""
     52 +|
 53  53  |    if choices_preview == "" do
 54  54  |      base
         |
 61  61  |  defp format_choice_with_def(choice, cells) do
 62  62  |    lemma = choice[:lemma] || ""
 63     -|    id    = choice[:id]
     63 +|    id = choice[:id]
 64  64  |    score = fmt_score(choice[:score])
 65  65  |
         |
 76  76  |
 77  77  |  defp definition_for_choice(choice, cells) do
 78     -|    by_id   = index_cells_by_id(cells)
     78 +|    by_id = index_cells_by_id(cells)
 79  79  |    by_norm = index_cells_by_norm(cells)
 80  80  |
 81     -|    id         = choice[:id]
 82     -|    alt_ids    = List.wrap(choice[:alt_ids])
     81 +|    id = choice[:id]
     82 +|    alt_ids = List.wrap(choice[:alt_ids])
 83  83  |    lemma_norm = norm_text(choice[:lemma] || "")
 84  84  |
         |
105 105  |            # 3a) same norm as chosen id (parsed from id)
106 106  |            idn = id_norm(id)
    107 +|
107 108  |            with_same_norm =
108 109  |              by_norm
         |
135 136  |
136 137  |  defp first_def_from_ids([], _by_id), do: ""
    138 +|
137 139  |  defp first_def_from_ids([h | t], by_id) do
138 140  |    case by_id |> Map.get(h) |> cell_def() |> gloss() do
139 141  |      "" -> first_def_from_ids(t, by_id)
140     -|      d  -> d
    142 +|      d -> d
141 143  |    end
142 144  |  end
         |
146 148  |      case cell_id(c) do
147 149  |        nil -> acc
148     -|        id  -> Map.put(acc, id, c)
    150 +|        id -> Map.put(acc, id, c)
149 151  |      end
150 152  |    end)
         |
155 157  |      case cell_norm(c) do
156 158  |        nil -> acc
157     -|        n   -> Map.put_new(acc, n, c)  # keep first with that norm
    159 +|        # keep first with that norm
    160 +|        n -> Map.put_new(acc, n, c)
158 161  |      end
159 162  |    end)
         |
167 170  |    end
168 171  |  end
    172 +|
169 173  |  defp cell_id(_), do: nil
170 174  |
         |
178 182  |    end
179 183  |  end
    184 +|
180 185  |  defp cell_norm(_), do: nil
181 186  |
         |
185 190  |  defp cell_def(c) when is_map(c),
186 191  |    do: Map.get(c, :definition) || Map.get(c, "definition")
    192 +|
187 193  |  defp cell_def(_), do: nil
188 194  |
189 195  |  defp gloss(nil), do: ""
190 196  |  defp gloss(""), do: ""
    197 +|
191 198  |  defp gloss(str) when is_binary(str) do
192 199  |    s = str |> String.replace(~r/\s+/u, " ") |> String.trim()
193     -|    if String.length(s) <= @def_char_limit, do: s, else: String.slice(s, 0, @def_char_limit) <> "…"
    200 +|
    201 +|    if String.length(s) <= @def_char_limit,
    202 +|      do: s,
    203 +|      else: String.slice(s, 0, @def_char_limit) <> "…"
194 204  |  end
195 205  |
         |
202 212  |  defp norm_text(v) when is_binary(v),
203 213  |    do: v |> String.downcase() |> String.replace(~r/\s+/u, " ") |> String.trim()
    214 +|
204 215  |  defp norm_text(v),
205 216  |    do:
         |
216 227  |        %{senses: [s | _]} ->
217 228  |          (s[:definition] || s[:def] || "") |> gloss()
    229 +|
218 230  |        _ ->
219 231  |          ""
         |
225 237  |    end
226 238  |  end
    239 +|
227 240  |  defp lexicon_def(_), do: ""
228 241  |
         |
275 288  |      task =
276 289  |        Task.Supervisor.async_nolink(Symbrella.TaskSup, fn ->
277     -|         
278     -|si = Core.resolve_input(text,
279     -|  mode: :prod,
280     -|  enrich_lexicon?: true,
281     -|  lexicon_stage?: true
282     -|)
283     -| 
    290 +|          si =
    291 +|            Core.resolve_input(text,
    292 +|              mode: :prod,
    293 +|              enrich_lexicon?: true,
    294 +|              lexicon_stage?: true
    295 +|            )
    296 +|
284 297  |          # Return a small, friendly UI summary
285 298  |          %{text: format_si_reply(si)}
         |
375 388  |        </div>
376 389  |      </header>
377     -|
378     -|      <!-- MESSAGES -->
    390 +|      
    391 +|    <!-- MESSAGES -->
379 392  |      <main
380 393  |        id="messages"
         |
414 427  |        </div>
415 428  |      </main>
416     -|
417     -|      <!-- COMPOSER -->
    429 +|      
    430 +|    <!-- COMPOSER -->
418 431  |      <footer
419 432  |        id="chat-composer"
         |
449 462  |end
450 463  |
451     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/db/lib/db/my_embeddibgs.ex
[0m
       |
 2  2  |  @dim Application.compile_env(:db, :embedding_dim, 1536)
 3  3  |
 4    -|def embed(text) when is_binary(text) do
    4 +|  def embed(text) when is_binary(text) do
 5  5  |    # your real implementation (or the deterministic test one)
 6  6  |    vec = for i <- 1..@dim, do: :math.sin(byte_size(text) + i)
       |
16 16  |end
17 17  |
18    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/test/brain/lifg_tripwire_test.exs
[0m
       |
49 49  |    # Telemetry proves the drop reason
50 50  |    assert_receive {:telemetry, [:brain, :lifg, :chargram_violation],
51    -|                    %{token_index: 0, phrase: "ck t"}, _}, 100
   51 +|                    %{token_index: 0, phrase: "ck t"}, _},
   52 +|                   100
52 53  |  end
53 54  |
       |
65 66  |end
66 67  |
67    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/db/lib/db.ex
[0m
       |
29 29  |  Returns `{:ok, %{rows: rows, missing_norms: missing, db_hits: hits}}`.
30 30  |  """
31    -|# move defaults to a head
32    -|@spec ltm(map(), keyword()) ::
33    -|        {:ok, %{rows: [Db.BrainCell.t()], missing_norms: [binary()], db_hits: MapSet.t(binary())}}
34    -|def ltm(si, opts \\ [])
   31 +|  # move defaults to a head
   32 +|  @spec ltm(map(), keyword()) ::
   33 +|          {:ok,
   34 +|           %{rows: [Db.BrainCell.t()], missing_norms: [binary()], db_hits: MapSet.t(binary())}}
   35 +|  def ltm(si, opts \\ [])
35 36  |
36    -|# primary clause
37    -|def ltm(%{tokens: tokens} = _si, _opts) when is_list(tokens) do
38    -|  norms =
39    -|    tokens
40    -|    |> Enum.map(&token_to_phrase/1)
41    -|    |> Enum.map(&norm/1)
42    -|    |> Enum.reject(&(&1 == ""))
43    -|    |> Enum.uniq()
   37 +|  # primary clause
   38 +|  def ltm(%{tokens: tokens} = _si, _opts) when is_list(tokens) do
   39 +|    norms =
   40 +|      tokens
   41 +|      |> Enum.map(&token_to_phrase/1)
   42 +|      |> Enum.map(&norm/1)
   43 +|      |> Enum.reject(&(&1 == ""))
   44 +|      |> Enum.uniq()
44 45  |
45    -|  if norms == [] do
46    -|    {:ok, %{rows: [], missing_norms: [], db_hits: MapSet.new()}}
47    -|  else
48    -|    rows =
49    -|      from(b in Db.BrainCell, where: b.norm in ^norms)
50    -|      |> Db.all()
   46 +|    if norms == [] do
   47 +|      {:ok, %{rows: [], missing_norms: [], db_hits: MapSet.new()}}
   48 +|    else
   49 +|      rows =
   50 +|        from(b in Db.BrainCell, where: b.norm in ^norms)
   51 +|        |> Db.all()
51 52  |
52    -|    existing = MapSet.new(for r <- rows, do: r.norm)
53    -|    missing  = Enum.reject(norms, &MapSet.member?(existing, &1))
54    -|    hits     = MapSet.new(for r <- rows, do: r.norm)
55    -|    {:ok, %{rows: rows, missing_norms: missing, db_hits: hits}}
   53 +|      existing = MapSet.new(for r <- rows, do: r.norm)
   54 +|      missing = Enum.reject(norms, &MapSet.member?(existing, &1))
   55 +|      hits = MapSet.new(for r <- rows, do: r.norm)
   56 +|      {:ok, %{rows: rows, missing_norms: missing, db_hits: hits}}
   57 +|    end
56 58  |  end
57    -|end
58 59  |
59    -|# fallback
60    -|def ltm(_si, _opts), do: {:ok, %{rows: [], missing_norms: [], db_hits: MapSet.new()}}
   60 +|  # fallback
   61 +|  def ltm(_si, _opts), do: {:ok, %{rows: [], missing_norms: [], db_hits: MapSet.new()}}
61 62  |
62 63  |  @doc """
       |
88 89  |  defp norm(_), do: ""
89 90  |end
90    -| 

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/lib/brain/ptmg.ex
[0m
         |
 97  97  |       opts: %{mode: mode, window_keep: keep} |> Map.merge(Map.new(opts)),
 98  98  |       window_keep: keep,
 99     -|       window: [],   # [{ts_ms, %{needy_count: n, queries: [...]}}]
100     -|       last: nil     # %{si, queries, evidence, audit}
     99 +|       # [{ts_ms, %{needy_count: n, queries: [...]}}]
    100 +|       window: [],
    101 +|       # %{si, queries, evidence, audit}
    102 +|       last: nil
101 103  |     }}
102 104  |  end
         |
173 175  |
174 176  |    window =
175     -|      [{System.system_time(:millisecond), %{needy_count: length(need), queries: queries}} | state.window]
    177 +|      [
    178 +|        {System.system_time(:millisecond), %{needy_count: length(need), queries: queries}}
    179 +|        | state.window
    180 +|      ]
176 181  |      |> Enum.take(state.window_keep)
177 182  |
         |
238 243  |
239 244  |    window =
240     -|      [{System.system_time(:millisecond), %{needy_count: length(need), queries: queries}} | state.window]
    245 +|      [
    246 +|        {System.system_time(:millisecond), %{needy_count: length(need), queries: queries}}
    247 +|        | state.window
    248 +|      ]
241 249  |      |> Enum.take(state.window_keep)
242 250  |
         |
252 260  |        mode: mode,
253 261  |        rerun?: rerun?
254     -|      }},
255     -|     %{state | last: last, window: window}}
    262 +|      }}, %{state | last: last, window: window}}
256 263  |  end
257 264  |
         |
332 339  |  defp episodes_hits(nil, _limit), do: []
333 340  |  defp episodes_hits("", _limit), do: []
    341 +|
334 342  |  defp episodes_hits(lemma, limit) do
335 343  |    mod = Module.concat([Db, Episodes])
         |
349 357  |  defp lexicon_hits(nil, _limit), do: []
350 358  |  defp lexicon_hits("", _limit), do: []
    359 +|
351 360  |  defp lexicon_hits(lemma, limit) do
352 361  |    mods = [Module.concat([Db, Lexicon]), Lexicon]
         |
395 404  |  def enforce_sense_compatibility(evidence, tokens)
396 405  |      when is_list(evidence) and is_list(tokens) do
397     -|    tmap = Map.new(tokens, fn t ->
398     -|      {Map.get(t, :index) || Map.get(t, "index"), t}
399     -|    end)
    406 +|    tmap =
    407 +|      Map.new(tokens, fn t ->
    408 +|        {Map.get(t, :index) || Map.get(t, "index"), t}
    409 +|      end)
400 410  |
401 411  |    Enum.map(evidence, fn ev ->
         |
453 463  |
454 464  |    Enum.each(evidence, fn ev ->
455     -|      has_hits = (ev.episodes != [] or ev.lexicon != [])
    465 +|      has_hits = ev.episodes != [] or ev.lexicon != []
    466 +|
456 467  |      if has_hits and is_binary(ev.chosen_id) do
457 468  |        Brain.cell_cast(ev.chosen_id, {:activate, %{delta: boost}})
    469 +|
458 470  |        Enum.each(ev.alt_ids || [], fn aid ->
459 471  |          Brain.cell_cast(aid, {:activate, %{delta: inhib}})
         |
529 541  |      m = Map.get(ch, :margin, 1.0)
530 542  |      alts = Map.get(ch, :alt_ids, [])
531     -|      is_number(m) and m < (thr * 1.0) and is_list(alts) and length(alts) > 0
    543 +|      is_number(m) and m < thr * 1.0 and is_list(alts) and length(alts) > 0
532 544  |    end)
533 545  |  end
         |
573 585  |end
574 586  |
575     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/config/test.exs
[0m
       |
12 12  |  show_sensitive_data_on_connection_error: true
13 13  |
14    -|
15 14  |# We don't run a server during test. If one is required,
16 15  |# you can enable the server option below.
       |

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/test/brain/pmtg_sense_compat_test.exs
[0m
       |
22 22  |  test "keeps only MWE senses for MWE tokens; keeps single-word senses for 1-word tokens" do
23 23  |    tokens = [
24    -|      %{index: 0, phrase: "new york", n: 2},  # MWE
25    -|      %{index: 1, phrase: "bank", n: 1}       # single
   24 +|      # MWE
   25 +|      %{index: 0, phrase: "new york", n: 2},
   26 +|      # single
   27 +|      %{index: 1, phrase: "bank", n: 1}
26 28  |    ]
27 29  |
       |
32 34  |        lexicon: [
33 35  |          %{id: "ny|noun|0", lemma: "new york"},
34    -|          %{id: "york|noun|0", lemma: "york"} # should be dropped for MWE
   36 +|          # should be dropped for MWE
   37 +|          %{id: "york|noun|0", lemma: "york"}
35 38  |        ]
36 39  |      },
       |
40 43  |        lexicon: [
41 44  |          %{id: "bank|noun|0", lemma: "bank"},
42    -|          %{id: "river bank|noun|0", lemma: "river bank"} # should be dropped for single
   45 +|          # should be dropped for single
   46 +|          %{id: "river bank|noun|0", lemma: "river bank"}
43 47  |        ]
44 48  |      }
       |
80 84  |    # Telemetry proves we had to fallback
81 85  |    assert_receive {:telemetry, [:brain, :pmtg, :no_mwe_senses],
82    -|                    %{orig: 2, kept: 0, token_index: 0, phrase: "hot dog"}, _}, 100
   86 +|                    %{orig: 2, kept: 0, token_index: 0, phrase: "hot dog"}, _},
   87 +|                   100
83 88  |  end
84 89  |end
85 90  |
86    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/test/test_helper.exs
[0m
       |
14 14  |)
15 15  |
16    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/lib/brain.ex
[0m
         |
144 144  |  """
145 145  |  @spec lifg_stage1(map() | [map()], [number()], keyword()) ::
146     -|          {:ok, %{choices: [map()], boosts: [{binary(), number()}], inhibitions: [{binary(), number()}], audit: map()}}
    146 +|          {:ok,
    147 +|           %{
    148 +|             choices: [map()],
    149 +|             boosts: [{binary(), number()}],
    150 +|             inhibitions: [{binary(), number()}],
    151 +|             audit: map()
    152 +|           }}
147 153  |          | {:error, term()}
148 154  |  def lifg_stage1(si_or_candidates, context_vec, opts \\ []) when is_list(context_vec) do
         |
222 228  |    tokens1 =
223 229  |      if is_list(tokens0) do
224     -|        normalize_tokens(tokens0, sentence) # ensure phrase/span; sort by start when spans exist
    230 +|        # ensure phrase/span; sort by start when spans exist
    231 +|        normalize_tokens(tokens0, sentence)
225 232  |      else
226 233  |        []
         |
294 301  |    case margins do
295 302  |      [] -> 1.0
296     -|      _  -> Enum.min(margins) * 1.0
    303 +|      _ -> Enum.min(margins) * 1.0
297 304  |    end
298 305  |  end
         |
311 318  |      choices
312 319  |      |> Enum.filter(fn ch ->
313     -|        m    = Map.get(ch, :margin, 1.0)
    320 +|        m = Map.get(ch, :margin, 1.0)
314 321  |        alts = Map.get(ch, :alt_ids, [])
315 322  |        is_number(m) and m < needy_thr and is_list(alts) and length(alts) > 0
         |
440 447  |
441 448  |  defp do_find_from(_gsent, _target, pos, max_start, _plen) when pos > max_start, do: nil
    449 +|
442 450  |  defp do_find_from(gsent, target, pos, max_start, plen) do
443 451  |    slice = gsent |> Enum.slice(pos, plen) |> Enum.join() |> String.downcase()
         |
508 516  |  defp extract_items(%{} = si) do
509 517  |    case Map.get(si, :active_cells, []) do
510     -|      list when is_list(list) -> list
    518 +|      list when is_list(list) ->
    519 +|        list
    520 +|
511 521  |      other ->
512 522  |        Logger.warning("Brain.extract_items: :active_cells not a list (got #{inspect(other)})")
         |
536 546  |        end
537 547  |
538     -|      _ -> :ok
    548 +|      _ ->
    549 +|        :ok
539 550  |    end
540 551  |  end
         |
549 560  |        Enum.find_value(group, fn c ->
550 561  |          v = getf(c, :phrase) || getf(c, :word) || getf(c, :lemma)
    562 +|
551 563  |          cond do
552 564  |            is_binary(v) ->
553 565  |              vt = String.trim(v)
554 566  |              if vt != "", do: vt, else: nil
555     -|            true -> nil
    567 +|
    568 +|            true ->
    569 +|              nil
556 570  |          end
557 571  |        end) || "t#{idx}"
         |
681 695  |  end
682 696  |
683     -|  defp normalize_episode_mode(:off),   do: :off
684     -|  defp normalize_episode_mode(:sync),  do: :sync
    697 +|  defp normalize_episode_mode(:off), do: :off
    698 +|  defp normalize_episode_mode(:sync), do: :sync
685 699  |  defp normalize_episode_mode(_other), do: :async
686 700  |
         |
700 714  |    case result do
701 715  |      {:ok, _ep} ->
702     -|        :telemetry.execute([:brain, :episodes, :write], %{ok: 1}, %{mode: mode_tag(async_embedding?)})
    716 +|        :telemetry.execute([:brain, :episodes, :write], %{ok: 1}, %{
    717 +|          mode: mode_tag(async_embedding?)
    718 +|        })
703 719  |
704 720  |      {:error, reason} ->
         |
713 729  |  end
714 730  |
715     -|  defp mode_tag(true),  do: :async
    731 +|  defp mode_tag(true), do: :async
716 732  |  defp mode_tag(false), do: :sync
717 733  |
         |
722 738  |end
723 739  |
724     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/lexicon/lib/lexicon.ex
[0m
       |
19 19  |          {:ok, items} when is_list(items) ->
20 20  |            %{word: norm(word), senses: flatten_senses(items)}
   21 +|
21 22  |          _ ->
22 23  |            %{word: norm(word), senses: []}
       |
42 43  |    |> Enum.flat_map(fn entry ->
43 44  |      meanings = entry["meanings"] || []
   45 +|
44 46  |      Enum.flat_map(meanings, fn m ->
45 47  |        pos = m["partOfSpeech"] |> down()
       |
68 70  |end
69 71  |
70    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/test/brain/lifg_priming_integration_test.exs
[0m
       |
 7  7  |      sense_candidates: %{
 8  8  |        0 => [
 9    -|          %{id: "bank|financial", features: %{lex_fit: 0.5, rel_prior: 0.5, activation: 0.0, intent_bias: 0.0}},
10    -|          %{id: "bank|river",     features: %{lex_fit: 0.5, rel_prior: 0.5, activation: 0.0, intent_bias: 0.0}}
    9 +|          %{
   10 +|            id: "bank|financial",
   11 +|            features: %{lex_fit: 0.5, rel_prior: 0.5, activation: 0.0, intent_bias: 0.0}
   12 +|          },
   13 +|          %{
   14 +|            id: "bank|river",
   15 +|            features: %{lex_fit: 0.5, rel_prior: 0.5, activation: 0.0, intent_bias: 0.0}
   16 +|          }
11 17  |        ]
12 18  |      }
       |
14 20  |
15 21  |    # Use disambiguate_stage1 to exercise the new path (priming weight = 0.0)
16    -|    si2 = Brain.LIFG.disambiguate_stage1(si, weights: %{lex: 0.25, sim: 0.0, rel: 0.15, prag: 0.10, act: 0.10}, scores: :top2)
   22 +|    si2 =
   23 +|      Brain.LIFG.disambiguate_stage1(si,
   24 +|        weights: %{lex: 0.25, sim: 0.0, rel: 0.15, prag: 0.10, act: 0.10},
   25 +|        scores: :top2
   26 +|      )
   27 +|
17 28  |    [ev | _] = si2.trace
18 29  |    [%{chosen_id: winner}] = ev.choices
       |
23 34  |end
24 35  |
25    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/db/test/test_helper.exs
[0m
       |
 8  8  |Ecto.Adapters.SQL.Sandbox.mode(Db, :manual)
 9  9  |
10    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/test/core/semantic_input_sense_candidates_test.exs
[0m
         |
 18  18  |      si =
 19  19  |        SI.emit_sense_candidates(@si, 0, scored, "alpha",
 20     -|          margin: 0.15, # keep >= 1.00 - 0.15 => 0.85
     20 +|          # keep >= 1.00 - 0.15 => 0.85
     21 +|          margin: 0.15,
 21  22  |          top_k: 3
 22  23  |        )
         |
 30  31  |    test "dedups by id (keep highest score) and merges with existing up to top_k" do
 31  32  |      si1 =
 32     -|        SI.emit_sense_candidates(@si, 2, [
 33     -|          {"b|verb|1", 0.90},
 34     -|          {"c|verb|1", 0.89}
 35     -|        ], "beta",
 36     -|          margin: 0.25, top_k: 3
     33 +|        SI.emit_sense_candidates(
     34 +|          @si,
     35 +|          2,
     36 +|          [
     37 +|            {"b|verb|1", 0.90},
     38 +|            {"c|verb|1", 0.89}
     39 +|          ],
     40 +|          "beta",
     41 +|          margin: 0.25,
     42 +|          top_k: 3
 37  43  |        )
 38  44  |
 39  45  |      # second batch: improves c, adds d; ensure merge + dedup + resort + cap by top_k
 40  46  |      si2 =
 41     -|        SI.emit_sense_candidates(si1, 2, [
 42     -|          {"c|verb|1", 0.95}, # higher than previous
 43     -|          {"d|verb|1", 0.86}
 44     -|        ], "beta",
 45     -|          margin: 0.25, top_k: 3
     47 +|        SI.emit_sense_candidates(
     48 +|          si1,
     49 +|          2,
     50 +|          [
     51 +|            # higher than previous
     52 +|            {"c|verb|1", 0.95},
     53 +|            {"d|verb|1", 0.86}
     54 +|          ],
     55 +|          "beta",
     56 +|          margin: 0.25,
     57 +|          top_k: 3
 46  58  |        )
 47  59  |
         |
 56  68  |      # max=0.80; with margin 0.30 threshold is 0.50 — but min_score = 0.60 should drop 0.55
 57  69  |      si =
 58     -|        SI.emit_sense_candidates(@si, 1, [
 59     -|          {"x|adj|0", 0.80},
 60     -|          {"y|adj|0", 0.55}
 61     -|        ], "xi",
     70 +|        SI.emit_sense_candidates(
     71 +|          @si,
     72 +|          1,
     73 +|          [
     74 +|            {"x|adj|0", 0.80},
     75 +|            {"y|adj|0", 0.55}
     76 +|          ],
     77 +|          "xi",
 62  78  |          margin: 0.30,
 63  79  |          min_score: 0.60,
         |
 70  86  |    end
 71  87  |
 72     -|test "accepts %{id, score} maps and bare ids (score defaults to 0.0)" do
 73     -|  # Use a larger margin so the 0.0 bare-id candidate isn't filtered out.
 74     -|  si =
 75     -|    SI.emit_sense_candidates(@si, 3, [
 76     -|      %{id: "mwe one|mwe|0", score: 0.25},
 77     -|      "lonely|noun|0"
 78     -|    ], "lemma3",
 79     -|      margin: 1.0,   # <-- key change
 80     -|      top_k: 5
 81     -|    )
     88 +|    test "accepts %{id, score} maps and bare ids (score defaults to 0.0)" do
     89 +|      # Use a larger margin so the 0.0 bare-id candidate isn't filtered out.
     90 +|      si =
     91 +|        SI.emit_sense_candidates(
     92 +|          @si,
     93 +|          3,
     94 +|          [
     95 +|            %{id: "mwe one|mwe|0", score: 0.25},
     96 +|            "lonely|noun|0"
     97 +|          ],
     98 +|          "lemma3",
     99 +|          # <-- key change
    100 +|          margin: 1.0,
    101 +|          top_k: 5
    102 +|        )
 82 103  |
 83     -|  cands = SI.get_sense_candidates(si, 3)
 84     -|  assert Enum.map(cands, & &1.id) == ["mwe one|mwe|0", "lonely|noun|0"]
 85     -|  assert Enum.map(cands, & &1.score) == [0.25, 0.0]
 86     -|  assert Enum.all?(cands, &(&1.lemma == "lemma3"))
 87     -|end
    104 +|      cands = SI.get_sense_candidates(si, 3)
    105 +|      assert Enum.map(cands, & &1.id) == ["mwe one|mwe|0", "lonely|noun|0"]
    106 +|      assert Enum.map(cands, & &1.score) == [0.25, 0.0]
    107 +|      assert Enum.all?(cands, &(&1.lemma == "lemma3"))
    108 +|    end
 88 109  |
 89 110  |    test "merges per-index but keeps indices isolated" do
         |
101 122  |end
102 123  |
103     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/db/test/db/episodes_test.exs
[0m
         |
 75  75  |  test "recall_hybrid/3 (token + vector) ranks the matching episode highest" do
 76  76  |    # Non-matching background episode
 77     -|    {:ok, _} = Episodes.write_episode(%{tokens: ["car", "engine"], sentence: "car engine trouble"})
     77 +|    {:ok, _} =
     78 +|      Episodes.write_episode(%{tokens: ["car", "engine"], sentence: "car engine trouble"})
 78  79  |
 79  80  |    # Strong match episode
 80     -|    {:ok, %Episode{} = ep} = Episodes.write_episode(%{
 81     -|      tokens: ["order", "adapter", "maple", "ridge"],
 82     -|      sentence: "order adapter in maple ridge",
 83     -|      intent: "purchase"
 84     -|    })
     81 +|    {:ok, %Episode{} = ep} =
     82 +|      Episodes.write_episode(%{
     83 +|        tokens: ["order", "adapter", "maple", "ridge"],
     84 +|        sentence: "order adapter in maple ridge",
     85 +|        intent: "purchase"
     86 +|      })
 85  87  |
 86  88  |    cues = ["order", "adapter", "maple"]
         |
 99 101  |end
100 102  |
101     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/symbrella_web/lib/symbrella_web/live/brain_live.ex
[0m
         |
  8   8  |
  9   9  |  @palette %{
 10     -|    lifg:        "#8b5cf6", # violet
 11     -|    atl:         "#ef4444", # red
 12     -|    mtg_sts:     "#f59e0b", # amber
 13     -|    tpj_ag:      "#06b6d4", # cyan
 14     -|    mpfc:        "#ec4899", # pink
 15     -|    ofc_vmpfc:   "#14b8a6", # teal
 16     -|    hippocampus: "#eab308", # yellow
 17     -|    thalamus:    "#64748b"  # slate
     10 +|    # violet
     11 +|    lifg: "#8b5cf6",
     12 +|    # red
     13 +|    atl: "#ef4444",
     14 +|    # amber
     15 +|    mtg_sts: "#f59e0b",
     16 +|    # cyan
     17 +|    tpj_ag: "#06b6d4",
     18 +|    # pink
     19 +|    mpfc: "#ec4899",
     20 +|    # teal
     21 +|    ofc_vmpfc: "#14b8a6",
     22 +|    # yellow
     23 +|    hippocampus: "#eab308",
     24 +|    # slate
     25 +|    thalamus: "#64748b"
 18  26  |  }
 19  27  |
         |
 26  34  |      desc:
 27  35  |        "Stage-1 sense selection & ambiguity pruning; early control/selection over candidate senses.",
 28     -|      d: "M 520 250 C 560 240, 600 220, 630 240 C 645 260, 640 290, 610 305 C 585 320, 545 320, 520 300 C 505 285, 505 265, 520 250 Z",
     36 +|      d:
     37 +|        "M 520 250 C 560 240, 600 220, 630 240 C 645 260, 640 290, 610 305 C 585 320, 545 320, 520 300 C 505 285, 505 265, 520 250 Z",
 29  38  |      anchor: {650, 258}
 30  39  |    },
         |
 33  42  |      label: "ATL (Anterior Temporal Lobe)",
 34  43  |      color: @palette.atl,
 35     -|      desc: "Hub-like semantic integration; converges modality-specific inputs at the temporal pole.",
 36     -|      d: "M 560 320 C 600 320, 640 330, 660 350 C 650 370, 620 385, 590 380 C 560 370, 540 350, 545 335 C 550 325, 555 322, 560 320 Z",
     44 +|      desc:
     45 +|        "Hub-like semantic integration; converges modality-specific inputs at the temporal pole.",
     46 +|      d:
     47 +|        "M 560 320 C 600 320, 640 330, 660 350 C 650 370, 620 385, 590 380 C 560 370, 540 350, 545 335 C 550 325, 555 322, 560 320 Z",
 37  48  |      anchor: {670, 350}
 38  49  |    },
         |
 42  53  |      color: @palette.mtg_sts,
 43  54  |      desc: "Lexical-semantic access & combinatorics; temporal dynamics for context fit.",
 44     -|      d: "M 420 300 C 460 290, 520 295, 560 310 C 560 330, 520 345, 470 340 C 440 335, 420 325, 420 300 Z",
     55 +|      d:
     56 +|        "M 420 300 C 460 290, 520 295, 560 310 C 560 330, 520 345, 470 340 C 440 335, 420 325, 420 300 Z",
 45  57  |      anchor: {585, 315}
 46  58  |    },
         |
 50  62  |      color: @palette.tpj_ag,
 51  63  |      desc: "Conceptual integration & controlled retrieval; discourse-level glue.",
 52     -|      d: "M 360 230 C 400 220, 450 225, 470 245 C 455 260, 430 275, 395 275 C 370 270, 355 255, 360 230 Z",
     64 +|      d:
     65 +|        "M 360 230 C 400 220, 450 225, 470 245 C 455 260, 430 275, 395 275 C 370 270, 355 255, 360 230 Z",
 53  66  |      anchor: {485, 245}
 54  67  |    },
         |
 58  71  |      color: @palette.mpfc,
 59  72  |      desc: "Goal/task schemas & global intent constraints over interpretation.",
 60     -|      d: "M 540 140 C 580 130, 620 130, 640 150 C 630 165, 600 175, 565 170 C 550 165, 542 155, 540 140 Z",
     73 +|      d:
     74 +|        "M 540 140 C 580 130, 620 130, 640 150 C 630 165, 600 175, 565 170 C 550 165, 542 155, 540 140 Z",
 61  75  |      anchor: {660, 150}
 62  76  |    },
         |
 65  79  |      label: "OFC / vmPFC",
 66  80  |      color: @palette.ofc_vmpfc,
 67     -|      desc: "Valuation & pragmatic bias; resolves among plausible readings via reward/utility signals.",
 68     -|      d: "M 560 200 C 600 205, 635 220, 650 240 C 635 250, 610 260, 580 260 C 560 255, 550 240, 552 220 Z",
     81 +|      desc:
     82 +|        "Valuation & pragmatic bias; resolves among plausible readings via reward/utility signals.",
     83 +|      d:
     84 +|        "M 560 200 C 600 205, 635 220, 650 240 C 635 250, 610 260, 580 260 C 560 255, 550 240, 552 220 Z",
 69  85  |      anchor: {665, 240}
 70  86  |    },
         |
 74  90  |      color: @palette.hippocampus,
 75  91  |      desc: "Associative recall & episode binding; links current semantics to episodic traces.",
 76     -|      d: "M 460 340 C 480 335, 505 340, 515 355 C 505 365, 485 370, 465 365 C 455 357, 455 347, 460 340 Z",
     92 +|      d:
     93 +|        "M 460 340 C 480 335, 505 340, 515 355 C 505 365, 485 370, 465 365 C 455 357, 455 347, 460 340 Z",
 77  94  |      anchor: {535, 355}
 78  95  |    },
         |
 82  99  |      color: @palette.thalamus,
 83 100  |      desc: "Relay/gating & rhythmic gain control across cortico-cortical loops.",
 84     -|      d: "M 410 250 C 430 245, 450 250, 455 265 C 445 275, 425 278, 410 270 C 405 262, 405 255, 410 250 Z",
    101 +|      d:
    102 +|        "M 410 250 C 430 245, 450 250, 455 265 C 445 275, 425 278, 410 270 C 405 262, 405 255, 410 250 Z",
 85 103  |      anchor: {470, 267}
 86 104  |    }
         |
127 145  |
128 146  |  defp label_color(hex) do
129     -|    with <<?#, r1::binary-size(2), g1::binary-size(2), b1::binary-size(2)>> <- String.downcase(hex),
    147 +|    with <<?#, r1::binary-size(2), g1::binary-size(2), b1::binary-size(2)>> <-
    148 +|           String.downcase(hex),
130 149  |         {r, ""} <- Integer.parse(r1, 16),
131 150  |         {g, ""} <- Integer.parse(g1, 16),
         |
148 167  |        <svg viewBox="0 0 800 480" class="w-full h-full">
149 168  |          <!-- Background -->
150     -|          <rect x="0" y="0" width="800" height="480" fill="transparent"/>
151     -|
152     -|          <!-- Brain silhouette (stylized, not too detailed) -->
    169 +|          <rect x="0" y="0" width="800" height="480" fill="transparent" />
    170 +|          
    171 +|    <!-- Brain silhouette (stylized, not too detailed) -->
153 172  |          <g>
154 173  |            <path
         |
163 182  |                 C 190 340, 170 325, 150 320
164 183  |                 C 120 310, 110 280, 120 260 Z"
165     -|              fill="#0b1220" fill-opacity="0.55"
166     -|              stroke="#0ea5e9" stroke-opacity="0.25" stroke-width="2"
    184 +|              fill="#0b1220"
    185 +|              fill-opacity="0.55"
    186 +|              stroke="#0ea5e9"
    187 +|              stroke-opacity="0.25"
    188 +|              stroke-width="2"
167 189  |            />
168 190  |            <!-- Brainstem (subtle) -->
169     -|            <path d="M 200 360 Q 210 390 245 410 Q 255 450 300 465"
170     -|                  fill="none" stroke="#0ea5e9" stroke-opacity="0.25" stroke-width="7" stroke-linecap="round"/>
    191 +|            <path
    192 +|              d="M 200 360 Q 210 390 245 410 Q 255 450 300 465"
    193 +|              fill="none"
    194 +|              stroke="#0ea5e9"
    195 +|              stroke-opacity="0.25"
    196 +|              stroke-width="7"
    197 +|              stroke-linecap="round"
    198 +|            />
171 199  |          </g>
172     -|
173     -|          <!-- Regions -->
    200 +|          
    201 +|    <!-- Regions -->
174 202  |          <%= for r <- @regions do %>
175 203  |            <% {fill_op, stroke_op} = opacities(r.id, @hovered, @selected) %>
176 204  |            <g
177     -|              phx-click="toggle" phx-value-id={r.id}
178     -|              phx-mouseenter="hover" phx-value-id={r.id}
    205 +|              phx-click="toggle"
    206 +|              phx-value-id={r.id}
    207 +|              phx-mouseenter="hover"
    208 +|              phx-value-id={r.id}
179 209  |              phx-mouseleave="leave"
180 210  |              class="cursor-pointer"
181 211  |            >
182     -|              <path d={r.d}
183     -|                    fill={r.color} fill-opacity={fill_op}
184     -|                    stroke={r.color} stroke-opacity={stroke_op} stroke-width="2.5"/>
    212 +|              <path
    213 +|                d={r.d}
    214 +|                fill={r.color}
    215 +|                fill-opacity={fill_op}
    216 +|                stroke={r.color}
    217 +|                stroke-opacity={stroke_op}
    218 +|                stroke-width="2.5"
    219 +|              />
185 220  |              <% {lx, ly} = r.anchor %>
186     -|              <line x1={lx - 18} y1={ly} x2={lx} y2={ly}
187     -|                    stroke={r.color} stroke-opacity="0.75" stroke-width="2"/>
188     -|              <rect x={lx + 2} y={ly - 14} rx="6" ry="6" width="220" height="28"
189     -|                    fill={r.color} fill-opacity="0.18" stroke={r.color} stroke-opacity="0.5" stroke-width="1"/>
190     -|              <text x={lx + 12} y={ly + 6} font-size="13" font-weight="600"
191     -|                    fill={label_color(r.color)} style="font-family: ui-sans-serif, system-ui;">
192     -|                <%= r.label %>
    221 +|              <line
    222 +|                x1={lx - 18}
    223 +|                y1={ly}
    224 +|                x2={lx}
    225 +|                y2={ly}
    226 +|                stroke={r.color}
    227 +|                stroke-opacity="0.75"
    228 +|                stroke-width="2"
    229 +|              />
    230 +|              <rect
    231 +|                x={lx + 2}
    232 +|                y={ly - 14}
    233 +|                rx="6"
    234 +|                ry="6"
    235 +|                width="220"
    236 +|                height="28"
    237 +|                fill={r.color}
    238 +|                fill-opacity="0.18"
    239 +|                stroke={r.color}
    240 +|                stroke-opacity="0.5"
    241 +|                stroke-width="1"
    242 +|              />
    243 +|              <text
    244 +|                x={lx + 12}
    245 +|                y={ly + 6}
    246 +|                font-size="13"
    247 +|                font-weight="600"
    248 +|                fill={label_color(r.color)}
    249 +|                style="font-family: ui-sans-serif, system-ui;"
    250 +|              >
    251 +|                {r.label}
193 252  |              </text>
194 253  |            </g>
195 254  |          <% end %>
196     -|
197     -|          <!-- Hover badge -->
    255 +|          
    256 +|    <!-- Hover badge -->
198 257  |          <%= if @hovered do %>
199 258  |            <% r = region_for(@hovered) %>
200 259  |            <% {bx, by} = r.anchor %>
201 260  |            <g>
202     -|              <rect x={bx - 140} y={max(by - 94, 12)} width="280" height="78" rx="10" ry="10"
203     -|                    fill="#0b1220" fill-opacity="0.95" stroke="#1f2937" stroke-width="1.25"/>
204     -|              <text x={bx - 128} y={max(by - 74, 32)} font-size="13" font-weight="700" fill="#e5e7eb"
205     -|                    style="font-family: ui-sans-serif, system-ui;"><%= r.label %></text>
206     -|              <text x={bx - 128} y={max(by - 56, 48)} font-size="12" fill="#cbd5e1"
207     -|                    style="font-family: ui-sans-serif, system-ui;">
208     -|                <tspan><%= String.slice(r.desc, 0, 52) %></tspan>
209     -|                <tspan x={bx - 128} dy="16"><%= String.slice(r.desc, 52, 52) %></tspan>
210     -|                <tspan x={bx - 128} dy="16"><%= String.slice(r.desc, 104, 52) %></tspan>
    261 +|              <rect
    262 +|                x={bx - 140}
    263 +|                y={max(by - 94, 12)}
    264 +|                width="280"
    265 +|                height="78"
    266 +|                rx="10"
    267 +|                ry="10"
    268 +|                fill="#0b1220"
    269 +|                fill-opacity="0.95"
    270 +|                stroke="#1f2937"
    271 +|                stroke-width="1.25"
    272 +|              />
    273 +|              <text
    274 +|                x={bx - 128}
    275 +|                y={max(by - 74, 32)}
    276 +|                font-size="13"
    277 +|                font-weight="700"
    278 +|                fill="#e5e7eb"
    279 +|                style="font-family: ui-sans-serif, system-ui;"
    280 +|              >
    281 +|                {r.label}
211 282  |              </text>
    283 +|              <text
    284 +|                x={bx - 128}
    285 +|                y={max(by - 56, 48)}
    286 +|                font-size="12"
    287 +|                fill="#cbd5e1"
    288 +|                style="font-family: ui-sans-serif, system-ui;"
    289 +|              >
    290 +|                <tspan>{String.slice(r.desc, 0, 52)}</tspan>
    291 +|                <tspan x={bx - 128} dy="16">{String.slice(r.desc, 52, 52)}</tspan>
    292 +|                <tspan x={bx - 128} dy="16">{String.slice(r.desc, 104, 52)}</tspan>
    293 +|              </text>
212 294  |            </g>
213 295  |          <% end %>
214 296  |        </svg>
215 297  |      </div>
216     -|
217     -|      <!-- Legend / details -->
    298 +|      
    299 +|    <!-- Legend / details -->
218 300  |      <aside class="rounded-2xl border border-slate-800/60 bg-[var(--color-panel)] p-4 space-y-4 shadow">
219 301  |        <h2 class="text-sm font-semibold opacity-80">Brain chain regions</h2>
         |
223 305  |            <% selected? = MapSet.member?(@selected, r.id) %>
224 306  |            <button
225     -|              phx-click="toggle" phx-value-id={r.id}
226     -|              phx-mouseenter="hover" phx-value-id={r.id}
    307 +|              phx-click="toggle"
    308 +|              phx-value-id={r.id}
    309 +|              phx-mouseenter="hover"
    310 +|              phx-value-id={r.id}
227 311  |              phx-mouseleave="leave"
228 312  |              class={[
         |
234 318  |              <div class="flex items-center gap-2">
235 319  |                <span class="inline-block w-3.5 h-3.5 rounded-full" style={"background: #{r.color}"} />
236     -|                <span class="text-sm font-medium"><%= r.label %></span>
    320 +|                <span class="text-sm font-medium">{r.label}</span>
237 321  |              </div>
238     -|              <p class="mt-1 text-xs opacity-80 leading-snug"><%= r.desc %></p>
    322 +|              <p class="mt-1 text-xs opacity-80 leading-snug">{r.desc}</p>
239 323  |            </button>
240 324  |          <% end %>
         |
250 334  |end
251 335  |
252     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/test/brain/lifg_scores_mode_test.exs
[0m
       |
46 46  |end
47 47  |
48    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/config/config.exs
[0m
       |
14 14  |  url: [host: "localhost"],
15 15  |  adapter: Bandit.PhoenixAdapter,
16    -|  render_errors: [formats: [html: SymbrellaWeb.ErrorHTML, json: SymbrellaWeb.ErrorJSON], layout: false],
   16 +|  render_errors: [
   17 +|    formats: [html: SymbrellaWeb.ErrorHTML, json: SymbrellaWeb.ErrorJSON],
   18 +|    layout: false
   19 +|  ],
17 20  |  pubsub_server: Symbrella.PubSub,
18 21  |  live_view: [signing_salt: "mkK1WujO"]
       |
31 34  |  version: "3.4.10",
32 35  |  default: [
33    -|    args: ~w(--config=tailwind.config.js --input=css/app.css --output=../priv/static/assets/app.css),
   36 +|    args:
   37 +|      ~w(--config=tailwind.config.js --input=css/app.css --output=../priv/static/assets/app.css),
34 38  |    cd: Path.expand("../apps/symbrella_web/assets", __DIR__)
35 39  |  ]
       |
55 59  |
56 60  |config :db, :embedding_dim, 1536
57    -|config :db, :embedder, MyEmbeddings  # implement MyEmbeddings.embed/1 -> {:ok, [float]}
   61 +|# implement MyEmbeddings.embed/1 -> {:ok, [float]}
   62 +|config :db, :embedder, MyEmbeddings
58 63  |
59 64  |# Core defaults
       |
66 71  |# ---- Brain (central defaults) ----
67 72  |config :brain,
68    -|  pmtg_mode: :boost,           # :boost | :rerun | :none
   73 +|  # :boost | :rerun | :none
   74 +|  pmtg_mode: :boost,
69 75  |  pmtg_margin_threshold: 0.15,
70 76  |  pmtg_window_keep: 50,
71    -| lifg_stage1_weights: %{lex_fit: 0.40, rel_prior: 0.35, activation: 0.15, intent_bias: 0.10},
   77 +|  lifg_stage1_weights: %{lex_fit: 0.40, rel_prior: 0.35, activation: 0.15, intent_bias: 0.10},
72 78  |  lifg_stage1_scores_mode: :all
   79 +|
73 80  |# Import env-specific at the end
74 81  |import_config "#{config_env()}.exs"
       |

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/lib/core/semantic_input.ex
[0m
         |
 30  30  |            trace: []
 31  31  |
     32 +|  @type sense_candidate :: %{id: String.t(), score: number(), lemma: String.t()}
 32  33  |
 33     -|@type sense_candidate :: %{id: String.t(), score: number(), lemma: String.t()}
     34 +|  @doc """
     35 +|  Record scored sense candidates for a token into `si.sense_candidates`.
 34  36  |
 35     -|@doc """
 36     -|Record scored sense candidates for a token into `si.sense_candidates`.
     37 +|  - `token_index` — index of the token in `si.tokens`.
     38 +|  - `scored` — list of `{id, score}` or `%{id: id, score: score}`.
     39 +|  - `lemma` — the token’s lemma (or downcased surface if you don’t have a lemma).
     40 +|  Options:
     41 +|    * `:margin`  — include near-winners within (max_score - margin). Default 0.15
     42 +|    * `:top_k`   — keep at most K per token after merge. Default 4
     43 +|    * `:min_score` — hard floor; drop anything below. Default nil (no floor)
     44 +|  """
     45 +|  @spec emit_sense_candidates(map(), non_neg_integer(), list(), String.t(), keyword()) :: map()
     46 +|  def emit_sense_candidates(%{} = si, token_index, scored, lemma, opts \\ []) do
     47 +|    margin = Keyword.get(opts, :margin, 0.15)
     48 +|    top_k = Keyword.get(opts, :top_k, 4)
     49 +|    min_score = Keyword.get(opts, :min_score, nil)
 37  50  |
 38     -|- `token_index` — index of the token in `si.tokens`.
 39     -|- `scored` — list of `{id, score}` or `%{id: id, score: score}`.
 40     -|- `lemma` — the token’s lemma (or downcased surface if you don’t have a lemma).
 41     -|Options:
 42     -|  * `:margin`  — include near-winners within (max_score - margin). Default 0.15
 43     -|  * `:top_k`   — keep at most K per token after merge. Default 4
 44     -|  * `:min_score` — hard floor; drop anything below. Default nil (no floor)
 45     -|"""
 46     -|@spec emit_sense_candidates(map(), non_neg_integer(), list(), String.t(), keyword()) :: map()
 47     -|def emit_sense_candidates(%{} = si, token_index, scored, lemma, opts \\ []) do
 48     -|  margin    = Keyword.get(opts, :margin, 0.15)
 49     -|  top_k     = Keyword.get(opts, :top_k, 4)
 50     -|  min_score = Keyword.get(opts, :min_score, nil)
     51 +|    list =
     52 +|      scored
     53 +|      |> Enum.map(fn
     54 +|        {id, score} -> %{id: id, score: score, lemma: lemma}
     55 +|        %{id: id, score: s} -> %{id: id, score: s, lemma: lemma}
     56 +|        id when is_binary(id) -> %{id: id, score: 0.0, lemma: lemma}
     57 +|      end)
 51  58  |
 52     -|  list =
 53     -|    scored
 54     -|    |> Enum.map(fn
 55     -|      {id, score}          -> %{id: id, score: score, lemma: lemma}
 56     -|      %{id: id, score: s}  -> %{id: id, score: s, lemma: lemma}
 57     -|      id when is_binary(id)-> %{id: id, score: 0.0, lemma: lemma}
 58     -|    end)
 59     -|
 60     -|  max_score =
 61     -|    case Enum.max_by(list, & &1.score, fn -> %{score: -1.0e9} end) do
 62     -|      %{score: s} -> s
 63     -|    end
 64     -|
 65     -|  filtered =
 66     -|    list
 67     -|    |> Enum.filter(fn %{score: s} ->
 68     -|      (is_nil(min_score) or s >= min_score) and s >= max_score - margin
 69     -|    end)
 70     -|    |> Enum.sort_by(& &1.score, :desc)
 71     -|    |> Enum.take(top_k)
 72     -|    # Dedup within this batch by id, keep highest score
 73     -|    |> Enum.reduce(%{}, fn %{id: id} = cand, acc ->
 74     -|      case acc do
 75     -|        %{^id => existing} -> if cand.score > existing.score, do: Map.put(acc, id, cand), else: acc
 76     -|        _ -> Map.put(acc, id, cand)
     59 +|    max_score =
     60 +|      case Enum.max_by(list, & &1.score, fn -> %{score: -1.0e9} end) do
     61 +|        %{score: s} -> s
 77  62  |      end
 78     -|    end)
 79     -|    |> Map.values()
 80     -|    |> Enum.sort_by(& &1.score, :desc)
 81  63  |
 82     -|  merged_per_idx =
 83     -|    si
 84     -|    |> Map.get(:sense_candidates, %{})
 85     -|    |> Map.update(token_index, filtered, fn existing ->
 86     -|      (existing ++ filtered)
     64 +|    filtered =
     65 +|      list
     66 +|      |> Enum.filter(fn %{score: s} ->
     67 +|        (is_nil(min_score) or s >= min_score) and s >= max_score - margin
     68 +|      end)
     69 +|      |> Enum.sort_by(& &1.score, :desc)
     70 +|      |> Enum.take(top_k)
     71 +|      # Dedup within this batch by id, keep highest score
 87  72  |      |> Enum.reduce(%{}, fn %{id: id} = cand, acc ->
 88  73  |        case acc do
 89     -|          %{^id => e} -> if cand.score > e.score, do: Map.put(acc, id, cand), else: acc
 90     -|          _ -> Map.put(acc, id, cand)
     74 +|          %{^id => existing} ->
     75 +|            if cand.score > existing.score, do: Map.put(acc, id, cand), else: acc
     76 +|
     77 +|          _ ->
     78 +|            Map.put(acc, id, cand)
 91  79  |        end
 92  80  |      end)
 93  81  |      |> Map.values()
 94  82  |      |> Enum.sort_by(& &1.score, :desc)
 95     -|      |> Enum.take(top_k)
 96     -|    end)
 97  83  |
 98     -|  Map.put(si, :sense_candidates, merged_per_idx)
 99     -|end
     84 +|    merged_per_idx =
     85 +|      si
     86 +|      |> Map.get(:sense_candidates, %{})
     87 +|      |> Map.update(token_index, filtered, fn existing ->
     88 +|        (existing ++ filtered)
     89 +|        |> Enum.reduce(%{}, fn %{id: id} = cand, acc ->
     90 +|          case acc do
     91 +|            %{^id => e} -> if cand.score > e.score, do: Map.put(acc, id, cand), else: acc
     92 +|            _ -> Map.put(acc, id, cand)
     93 +|          end
     94 +|        end)
     95 +|        |> Map.values()
     96 +|        |> Enum.sort_by(& &1.score, :desc)
     97 +|        |> Enum.take(top_k)
     98 +|      end)
100  99  |
101     -|@doc """
102     -|Get all sense candidates, or only those for a single token index.
103     -|"""
104     -|@spec get_sense_candidates(map(), non_neg_integer() | :all) :: list() | map()
105     -|def get_sense_candidates(%{} = si, idx \\ :all) do
106     -|  sc = Map.get(si, :sense_candidates, %{})
107     -|  case idx do
108     -|    :all -> sc
109     -|    _    -> Map.get(sc, idx, [])
    100 +|    Map.put(si, :sense_candidates, merged_per_idx)
110 101  |  end
111     -|end
112 102  |
    103 +|  @doc """
    104 +|  Get all sense candidates, or only those for a single token index.
    105 +|  """
    106 +|  @spec get_sense_candidates(map(), non_neg_integer() | :all) :: list() | map()
    107 +|  def get_sense_candidates(%{} = si, idx \\ :all) do
    108 +|    sc = Map.get(si, :sense_candidates, %{})
    109 +|
    110 +|    case idx do
    111 +|      :all -> sc
    112 +|      _ -> Map.get(sc, idx, [])
    113 +|    end
    114 +|  end
113 115  |end
114 116  |

[1m[31m/data/data/com.termux/files/home/Symbrella/config/runtime.exs
[0m
       |
 5  5  |  case System.get_env("PMTG_MODE", "boost") do
 6  6  |    "rerun" -> :rerun
 7    -|    "none"  -> :none
 8    -|    _       -> :boost
    7 +|    "none" -> :none
    8 +|    _ -> :boost
 9  9  |  end
10 10  |
       |
24 24  |lifg_weights =
25 25  |  case System.get_env("LIFG_WTS") do
26    -|    nil -> nil
   26 +|    nil ->
   27 +|      nil
   28 +|
27 29  |    csv ->
28 30  |      case String.split(csv, ",") |> Enum.map(&Float.parse/1) do
29    -|        [{lex,_},{rel,_},{act,_},{prag,_}] ->
   31 +|        [{lex, _}, {rel, _}, {act, _}, {prag, _}] ->
30 32  |          %{lex_fit: lex, rel_prior: rel, activation: act, intent_bias: prag}
31    -|        _ -> nil
   33 +|
   34 +|        _ ->
   35 +|          nil
32 36  |      end
33 37  |  end
       |
36 40  |lifg_scores_mode =
37 41  |  case String.downcase(System.get_env("LIFG_SCORES_MODE", "")) do
38    -|    "all"  -> :all
   42 +|    "all" -> :all
39 43  |    "top2" -> :top2
40 44  |    "none" -> :none
       |
70 74  |end
71 75  |
72    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/test/core/token_test.exs
[0m
       |
26 26  |end
27 27  |
28    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/test/brain/priming_test.exs
[0m
       |
18 18  |end
19 19  |
20    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/symbrella/lib/symbrella.ex
[0m
       |
87 87  |end
88 88  |
89    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/symbrella_web/mix.exs
[0m
       |
37 37  |  #
38 38  |  # Type `mix help deps` for examples and options.
39    -| defp deps do
40    -|   [
41    -|     {:phoenix, "~> 1.8.1"},
42    -|     {:phoenix_html, "~> 4.1"},
43    -|     {:phoenix_live_reload, "~> 1.2", only: :dev},
44    -|     {:phoenix_live_view, "~> 1.1.0"},
45    -|     {:lazy_html, ">= 0.1.0", only: :test},
46    -|     {:phoenix_live_dashboard, "~> 0.8.3"},
47    -|     {:esbuild, "~> 0.10", runtime: Mix.env() == :dev},
48    -|     {:tailwind, "~> 0.3", runtime: Mix.env() == :dev},
49    -|     {:heroicons,
50    -|      github: "tailwindlabs/heroicons",
51    -|      tag: "v2.2.0",
52    -|      sparse: "optimized",
53    -|      app: false,
54    -|      compile: false,
55    -|      depth: 1},
56    -|     {:telemetry_metrics, "~> 1.0"},
57    -|     {:telemetry_poller, "~> 1.0"},
58    -|     {:gettext, "~> 0.26"},
59    -|    {:symbrella, in_umbrella: true},  # keeps your TaskSup / app-wide bits
60    -|    {:core,      in_umbrella: true},  # ensures Core starts with the web
61    -|    {:brain,     in_umbrella: true},  # ensures Brain GenServer starts
62    -|    {:db,        in_umbrella: true},  # ensures Repo is up for Core/Brain
63    -|     {:jason, "~> 1.2"},
64    -|     {:bandit, "~> 1.5"}
65    -|   ]
66    -| end
   39 +|  defp deps do
   40 +|    [
   41 +|      {:phoenix, "~> 1.8.1"},
   42 +|      {:phoenix_html, "~> 4.1"},
   43 +|      {:phoenix_live_reload, "~> 1.2", only: :dev},
   44 +|      {:phoenix_live_view, "~> 1.1.0"},
   45 +|      {:lazy_html, ">= 0.1.0", only: :test},
   46 +|      {:phoenix_live_dashboard, "~> 0.8.3"},
   47 +|      {:esbuild, "~> 0.10", runtime: Mix.env() == :dev},
   48 +|      {:tailwind, "~> 0.3", runtime: Mix.env() == :dev},
   49 +|      {:heroicons,
   50 +|       github: "tailwindlabs/heroicons",
   51 +|       tag: "v2.2.0",
   52 +|       sparse: "optimized",
   53 +|       app: false,
   54 +|       compile: false,
   55 +|       depth: 1},
   56 +|      {:telemetry_metrics, "~> 1.0"},
   57 +|      {:telemetry_poller, "~> 1.0"},
   58 +|      {:gettext, "~> 0.26"},
   59 +|      # keeps your TaskSup / app-wide bits
   60 +|      {:symbrella, in_umbrella: true},
   61 +|      # ensures Core starts with the web
   62 +|      {:core, in_umbrella: true},
   63 +|      # ensures Brain GenServer starts
   64 +|      {:brain, in_umbrella: true},
   65 +|      # ensures Repo is up for Core/Brain
   66 +|      {:db, in_umbrella: true},
   67 +|      {:jason, "~> 1.2"},
   68 +|      {:bandit, "~> 1.5"}
   69 +|    ]
   70 +|  end
67 71  |
68 72  |  # Aliases are shortcuts or tasks specific to the current project.
       |

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/lib/core/lexicon/stage.ex
[0m
         |
 59  59  |        |> Enum.map(fn {s, i} ->
 60  60  |          pos = normalize_pos(get(s, :pos))
     61 +|
 61  62  |          %{
 62  63  |            id: "#{word}|#{pos}|#{i}",
         |
 71  72  |        end)
 72  73  |
 73     -|      _ -> []
     74 +|      _ ->
     75 +|        []
 74  76  |    end
 75  77  |  end
         |
 98 100  |end
 99 101  |
100     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/lib/core.ex
[0m
         |
 16  16  |  @spec resolve_input(String.t(), opts()) :: SemanticInput.t()
 17  17  |  def resolve_input(phrase, opts \\ []) when is_binary(phrase) do
 18     -|    mode  = Keyword.get(opts, :mode, :prod)
     18 +|    mode = Keyword.get(opts, :mode, :prod)
 19  19  |    max_n = Keyword.get(opts, :max_wordgram_n, 3)
 20  20  |
         |
 22  22  |      phrase
 23  23  |      |> Core.LIFG.Input.tokenize(max_wordgram_n: max_n)
 24     -|      |> wrap_si(phrase)                 # ensure %SemanticInput{}
     24 +|      # ensure %SemanticInput{}
     25 +|      |> wrap_si(phrase)
 25  26  |      |> rebuild_word_ngrams(max_n)
 26  27  |      |> Map.put(:source, if(mode == :prod, do: :prod, else: :test))
         |
 48  49  |
 49  50  |  # ─────────────────────── Brain notify ───────────────────────
 50     -|# add this helper near your other privates if you don’t already have it
 51     -|defp id_norm(nil), do: nil
 52     -|defp id_norm(id) when is_binary(id), do: id |> String.split("|") |> hd()
     51 +|  # add this helper near your other privates if you don’t already have it
     52 +|  defp id_norm(nil), do: nil
     53 +|  defp id_norm(id) when is_binary(id), do: id |> String.split("|") |> hd()
 53  54  |
 54     -|
 55  55  |  # Uses direct GenServer.cast/2 instead of any wrapper.
 56  56  |  defp notify_brain_activation(si, opts) do
         |
 80  80  |      |> Enum.reduce(%{}, fn t, acc ->
 81  81  |        nrm = norm(Map.get(t, :phrase))
 82     -|        tn  = Map.get(t, :n, 1)
     82 +|        tn = Map.get(t, :n, 1)
 83  83  |
 84  84  |        senses =
         |
108 108  |          })
109 109  |
110     -|lifg_choices =
111     -|  Enum.map(out.choices, fn ch ->
112     -|    token_norm = norm(Map.get(ch, :lemma) || "")
113     -|    chosen_id  = Map.get(ch, :chosen_id)
114     -|    scores     = Map.get(ch, :scores) || %{}
115     -|    feats      = Map.get(ch, :features) || %{}
116     -|    alt_ids    = Map.get(ch, :alt_ids, [])
    110 +|        lifg_choices =
    111 +|          Enum.map(out.choices, fn ch ->
    112 +|            token_norm = norm(Map.get(ch, :lemma) || "")
    113 +|            chosen_id = Map.get(ch, :chosen_id)
    114 +|            scores = Map.get(ch, :scores) || %{}
    115 +|            feats = Map.get(ch, :features) || %{}
    116 +|            alt_ids = Map.get(ch, :alt_ids, [])
117 117  |
118     -|    base_score =
119     -|      if is_binary(chosen_id) and is_map(scores),
120     -|        do: Map.get(scores, chosen_id, 0.0),
121     -|        else: Map.get(feats, :score_norm, 0.0)
    118 +|            base_score =
    119 +|              if is_binary(chosen_id) and is_map(scores),
    120 +|                do: Map.get(scores, chosen_id, 0.0),
    121 +|                else: Map.get(feats, :score_norm, 0.0)
122 122  |
123     -|    # Prefer an id whose norm matches the token’s norm, choosing the highest score among matches.
124     -|    # Consider both the chosen_id and alt_ids.
125     -|    candidates = [chosen_id | alt_ids]
    123 +|            # Prefer an id whose norm matches the token’s norm, choosing the highest score among matches.
    124 +|            # Consider both the chosen_id and alt_ids.
    125 +|            candidates = [chosen_id | alt_ids]
126 126  |
127     -|    matching =
128     -|      candidates
129     -|      |> Enum.filter(& (id_norm(&1) == token_norm))
    127 +|            matching =
    128 +|              candidates
    129 +|              |> Enum.filter(&(id_norm(&1) == token_norm))
130 130  |
131     -|    {chosen_id2, score2} =
132     -|      case matching do
133     -|        [] ->
134     -|          {chosen_id, base_score}
135     -|        matches ->
136     -|          Enum.max_by(matches, fn id -> Map.get(scores, id, -1.0) end, fn -> chosen_id end)
137     -|          |> then(fn best -> {best, Map.get(scores, best, base_score)} end)
138     -|      end
    131 +|            {chosen_id2, score2} =
    132 +|              case matching do
    133 +|                [] ->
    134 +|                  {chosen_id, base_score}
139 135  |
140     -|    %{
141     -|      token_index: Map.get(ch, :token_index),
142     -|      lemma: token_norm,
143     -|      id: chosen_id2,
144     -|      alt_ids: alt_ids,
145     -|      score: score2
146     -|    }
147     -|  end)
    136 +|                matches ->
    137 +|                  Enum.max_by(matches, fn id -> Map.get(scores, id, -1.0) end, fn -> chosen_id end)
    138 +|                  |> then(fn best -> {best, Map.get(scores, best, base_score)} end)
    139 +|              end
148 140  |
    141 +|            %{
    142 +|              token_index: Map.get(ch, :token_index),
    143 +|              lemma: token_norm,
    144 +|              id: chosen_id2,
    145 +|              alt_ids: alt_ids,
    146 +|              score: score2
    147 +|            }
    148 +|          end)
    149 +|
149 150  |        si
150 151  |        |> Map.put(:lifg_choices, lifg_choices)
         |
156 157  |  end
157 158  |
158     -|defp maybe_ingest_atl(%{lifg_choices: choices, tokens: tokens} = si, _opts)
159     -|     when is_list(choices) and is_list(tokens) do
160     -|  if choices == [] do
161     -|    si
162     -|  else
163     -|    slate =
164     -|      case Process.whereis(Brain.ATL) do
165     -|        pid when is_pid(pid) ->
166     -|          # server path
167     -|          Brain.ATL.ingest(choices, tokens)
168     -|        _ ->
169     -|          # pure fallback path
170     -|          Brain.ATL.reduce(choices, tokens)
171     -|      end
    159 +|  defp maybe_ingest_atl(%{lifg_choices: choices, tokens: tokens} = si, _opts)
    160 +|       when is_list(choices) and is_list(tokens) do
    161 +|    if choices == [] do
    162 +|      si
    163 +|    else
    164 +|      slate =
    165 +|        case Process.whereis(Brain.ATL) do
    166 +|          pid when is_pid(pid) ->
    167 +|            # server path
    168 +|            Brain.ATL.ingest(choices, tokens)
172 169  |
173     -|    si
174     -|    |> Map.put(:atl_slate, slate)
175     -|    |> Map.update(:trace, [], fn tr ->
176     -|      [
177     -|        %{
178     -|          stage: :atl,
179     -|          ts_ms: System.system_time(:millisecond),
180     -|          winners: Map.get(slate, :winner_count, 0),
181     -|          concepts: slate |> Map.get(:by_norm, %{}) |> map_size()
182     -|        }
183     -|        | tr
184     -|      ]
185     -|    end)
    170 +|          _ ->
    171 +|            # pure fallback path
    172 +|            Brain.ATL.reduce(choices, tokens)
    173 +|        end
    174 +|
    175 +|      si
    176 +|      |> Map.put(:atl_slate, slate)
    177 +|      |> Map.update(:trace, [], fn tr ->
    178 +|        [
    179 +|          %{
    180 +|            stage: :atl,
    181 +|            ts_ms: System.system_time(:millisecond),
    182 +|            winners: Map.get(slate, :winner_count, 0),
    183 +|            concepts: slate |> Map.get(:by_norm, %{}) |> map_size()
    184 +|          }
    185 +|          | tr
    186 +|        ]
    187 +|      end)
    188 +|    end
186 189  |  end
187     -|end
188 190  |
189     -|defp maybe_ingest_atl(si, _opts), do: si
    191 +|  defp maybe_ingest_atl(si, _opts), do: si
190 192  |
191 193  |  # Only allow unigrams to consider unigram senses, and MWEs to consider MWEs.
192 194  |  defp compatible_cell_for_token?(token_n, cell) do
193     -|    nrm = (Map.get(cell, :norm) || Map.get(cell, "norm") || "")
    195 +|    nrm = Map.get(cell, :norm) || Map.get(cell, "norm") || ""
194 196  |    has_space = String.contains?(nrm, " ")
195 197  |    (token_n > 1 and has_space) or (token_n == 1 and not has_space)
         |
208 210  |  end
209 211  |
210     -|defp maybe_encode_hippocampus(%{atl_slate: slate} = si) when is_map(slate) do
211     -|  if Process.whereis(Brain.Hippocampus) do
212     -|    ep = Brain.Hippocampus.encode(slate)
213     -|    # keep a tiny summary on SI; full episode lives in Hippocampus state
214     -|    Map.put(si, :episode, Map.take(ep, [:ts_ms, :token_count, :winner_count]))
215     -|  else
216     -|    si
    212 +|  defp maybe_encode_hippocampus(%{atl_slate: slate} = si) when is_map(slate) do
    213 +|    if Process.whereis(Brain.Hippocampus) do
    214 +|      ep = Brain.Hippocampus.encode(slate)
    215 +|      # keep a tiny summary on SI; full episode lives in Hippocampus state
    216 +|      Map.put(si, :episode, Map.take(ep, [:ts_ms, :token_count, :winner_count]))
    217 +|    else
    218 +|      si
    219 +|    end
217 220  |  end
218     -|end
219     -|defp maybe_encode_hippocampus(si), do: si
220 221  |
    222 +|  defp maybe_encode_hippocampus(si), do: si
    223 +|
221 224  |  defp seed?(s), do: (Map.get(s, :type) || Map.get(s, "type")) == "seed"
222 225  |
         |
368 371  |
369 372  |  # used when merging active_cells
370     -|  defp sanitize_cell(%BrainCell{} = s),      do: [s]
371     -|  defp sanitize_cell(%{id: _} = m),          do: [m]
372     -|  defp sanitize_cell(%{"id" => _} = m),      do: [m]
    373 +|  defp sanitize_cell(%BrainCell{} = s), do: [s]
    374 +|  defp sanitize_cell(%{id: _} = m), do: [m]
    375 +|  defp sanitize_cell(%{"id" => _} = m), do: [m]
373 376  |  defp sanitize_cell(id) when is_binary(id), do: [%{id: id}]
374     -|  defp sanitize_cell(_),                     do: []
    377 +|  defp sanitize_cell(_), do: []
375 378  |
376     -|  defp cell_id(%BrainCell{id: id}),    do: id
377     -|  defp cell_id(%{id: id}),             do: id
378     -|  defp cell_id(%{"id" => id}),         do: id
    379 +|  defp cell_id(%BrainCell{id: id}), do: id
    380 +|  defp cell_id(%{id: id}), do: id
    381 +|  defp cell_id(%{"id" => id}), do: id
379 382  |  defp cell_id(id) when is_binary(id), do: id
380     -|  defp cell_id(_),                     do: nil
    383 +|  defp cell_id(_), do: nil
381 384  |
382 385  |  defp norm(nil), do: ""
         |
394 397  |end
395 398  |
396     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/lib/core/brain.ex
[0m
       |
19 19  |end
20 20  |
21    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/lib/brain/atl.ex
[0m
         |
 56  56  |       keep: keep,
 57  57  |       last_slate: %{},
 58     -|       concept_counts: %{}, # norm => count (concept-level)
 59     -|       sense_counts: %{},   # id   => count (sense-level)
 60     -|       window: []           # [{ts_ms, slate}, ...]
     58 +|       # norm => count (concept-level)
     59 +|       concept_counts: %{},
     60 +|       # id   => count (sense-level)
     61 +|       sense_counts: %{},
     62 +|       # [{ts_ms, slate}, ...]
     63 +|       window: []
 61  64  |     }}
 62  65  |  end
         |
114 117  |      choices
115 118  |      |> Enum.map(&normalize_choice/1)
116     -|      |> Enum.reject(&is_nil(&1.id)) # only keep items with an id
    119 +|      # only keep items with an id
    120 +|      |> Enum.reject(&is_nil(&1.id))
117 121  |
118 122  |    by_norm = Enum.group_by(winners, & &1.norm)
119     -|    by_id   = Enum.group_by(winners, & &1.id)
    123 +|    by_id = Enum.group_by(winners, & &1.id)
120 124  |
121 125  |    %{
         |
134 138  |  # and simpler %{id:, lemma: ...}. Falls back where possible.
135 139  |  defp normalize_choice(ch) when is_map(ch) do
136     -|    id         = fetch_any(ch, [:chosen_id, "chosen_id", :id, "id"])
137     -|    token_idx  = fetch_any(ch, [:token_index, "token_index"])
    140 +|    id = fetch_any(ch, [:chosen_id, "chosen_id", :id, "id"])
    141 +|    token_idx = fetch_any(ch, [:token_index, "token_index"])
138 142  |    score_norm = fetch_from_scores(ch, id) || fetch_any(ch, [:score, "score"]) || 0.0
139     -|    margin     = fetch_any(ch, [:margin, "margin"]) || 0.0
140     -|    lemma0     = fetch_any(ch, [:lemma, "lemma"]) |> norm_text()
    143 +|    margin = fetch_any(ch, [:margin, "margin"]) || 0.0
    144 +|    lemma0 = fetch_any(ch, [:lemma, "lemma"]) |> norm_text()
141 145  |
142 146  |    %{
         |
158 162  |      case Map.get(map, k) do
159 163  |        nil -> {:cont, nil}
160     -|        v   -> {:halt, v}
    164 +|        v -> {:halt, v}
161 165  |      end
162 166  |    end)
         |
171 175  |
172 176  |  defp norm_text(nil), do: ""
    177 +|
173 178  |  defp norm_text(v) when is_binary(v),
174 179  |    do: v |> String.downcase() |> String.replace(~r/\s+/u, " ") |> String.trim()
    180 +|
175 181  |  defp norm_text(v),
176 182  |    do:
         |
181 187  |      |> String.trim()
182 188  |
183     -|# ───────── Sense slate promotion → si.sense_candidates ─────────
    189 +|  # ───────── Sense slate promotion → si.sense_candidates ─────────
184 190  |
185     -|@doc """
186     -|Attach sense candidates to the Semantic Input (SI), keyed by token index.
    191 +|  @doc """
    192 +|  Attach sense candidates to the Semantic Input (SI), keyed by token index.
187 193  |
188     -|- Pulls winners and near-winners from the current ATL `slate`
189     -|- Uses `raw.scores` when available to expand alt candidates
190     -|- Keeps a compact, LIFG-ready payload: %{token_index => [%{id, score, ...}]}
    194 +|  - Pulls winners and near-winners from the current ATL `slate`
    195 +|  - Uses `raw.scores` when available to expand alt candidates
    196 +|  - Keeps a compact, LIFG-ready payload: %{token_index => [%{id, score, ...}]}
191 197  |
192     -|Opts:
193     -|  * :top_k         — default 3
194     -|  * :margin_window — default 0.05 (accept alts within 5% of winner score)
195     -|"""
196     -|@spec attach_sense_candidates(map(), map(), keyword()) :: map()
197     -|def attach_sense_candidates(si, slate, opts \\ []) when is_map(si) and is_map(slate) do
198     -|  candidates = promote_sense_candidates_from_slate(slate, opts)
199     -|  Map.put(si, :sense_candidates, candidates)
200     -|end
    198 +|  Opts:
    199 +|    * :top_k         — default 3
    200 +|    * :margin_window — default 0.05 (accept alts within 5% of winner score)
    201 +|  """
    202 +|  @spec attach_sense_candidates(map(), map(), keyword()) :: map()
    203 +|  def attach_sense_candidates(si, slate, opts \\ []) when is_map(si) and is_map(slate) do
    204 +|    candidates = promote_sense_candidates_from_slate(slate, opts)
    205 +|    Map.put(si, :sense_candidates, candidates)
    206 +|  end
201 207  |
202     -|@spec promote_sense_candidates_from_slate(map(), keyword()) ::
203     -|        %{non_neg_integer() => [map()]}
204     -|def promote_sense_candidates_from_slate(%{winners: winners} = slate, opts) do
205     -|  top_k         = Keyword.get(opts, :top_k, 3)
206     -|  margin_window = Keyword.get(opts, :margin_window, 0.05)
    208 +|  @spec promote_sense_candidates_from_slate(map(), keyword()) ::
    209 +|          %{non_neg_integer() => [map()]}
    210 +|  def promote_sense_candidates_from_slate(%{winners: winners} = slate, opts) do
    211 +|    top_k = Keyword.get(opts, :top_k, 3)
    212 +|    margin_window = Keyword.get(opts, :margin_window, 0.05)
207 213  |
208     -|  # Build per-token candidate lists from winners + raw.scores (if present)
209     -|  winners
210     -|  |> Enum.group_by(& &1.token_index)
211     -|  |> Enum.into(%{}, fn {idx, entries} ->
212     -|    # Winner for this token is entries |> Enum.max_by(&score) in most setups,
213     -|    # but your slate already has per-token winner at head; keep head as winner.
214     -|    winner = List.first(entries) || %{}
215     -|    w_score =
216     -|      winner[:score] ||
217     -|        get_in(winner, [:raw, :score_norm]) ||
218     -|        0.0
    214 +|    # Build per-token candidate lists from winners + raw.scores (if present)
    215 +|    winners
    216 +|    |> Enum.group_by(& &1.token_index)
    217 +|    |> Enum.into(%{}, fn {idx, entries} ->
    218 +|      # Winner for this token is entries |> Enum.max_by(&score) in most setups,
    219 +|      # but your slate already has per-token winner at head; keep head as winner.
    220 +|      winner = List.first(entries) || %{}
219 221  |
220     -|    # Expand alternatives from raw.scores if present; otherwise just keep the winner.
221     -|    alts =
222     -|      winner
223     -|      |> Map.get(:raw, %{})
224     -|      |> Map.get(:scores, %{})
225     -|      |> Enum.map(fn {id, s} ->
226     -|        %{
227     -|          id: id,
228     -|          score: as_float(s),
229     -|          rank: nil,
230     -|          from: :atl_scores,
231     -|          pos: pos_from_id(id),
232     -|          norm: Map.get(winner, :norm),
233     -|          lemma: Map.get(winner, :lemma),
234     -|          features: Map.get(winner, :raw, %{}) |> Map.get(:features, %{}),
235     -|          margin: nil
236     -|        }
237     -|      end)
    222 +|      w_score =
    223 +|        winner[:score] ||
    224 +|          get_in(winner, [:raw, :score_norm]) ||
    225 +|          0.0
238 226  |
239     -|    # Include the winner explicitly (first, de-duplicated)
240     -|    winner_as_candidate = %{
241     -|      id: winner[:id],
242     -|      score: as_float(w_score),
243     -|      rank: 0,
244     -|      from: :atl_winner,
245     -|      pos: pos_from_id(winner[:id]),
246     -|      norm: winner[:norm],
247     -|      lemma: winner[:lemma],
248     -|      features: Map.get(winner, :raw, %{}) |> Map.get(:features, %{}),
249     -|      margin: Map.get(winner, :margin, 0.0)
250     -|    }
    227 +|      # Expand alternatives from raw.scores if present; otherwise just keep the winner.
    228 +|      alts =
    229 +|        winner
    230 +|        |> Map.get(:raw, %{})
    231 +|        |> Map.get(:scores, %{})
    232 +|        |> Enum.map(fn {id, s} ->
    233 +|          %{
    234 +|            id: id,
    235 +|            score: as_float(s),
    236 +|            rank: nil,
    237 +|            from: :atl_scores,
    238 +|            pos: pos_from_id(id),
    239 +|            norm: Map.get(winner, :norm),
    240 +|            lemma: Map.get(winner, :lemma),
    241 +|            features: Map.get(winner, :raw, %{}) |> Map.get(:features, %{}),
    242 +|            margin: nil
    243 +|          }
    244 +|        end)
251 245  |
252     -|    # Filter near-winners by margin window around the winner score, take top_k
253     -|    near =
254     -|      alts
255     -|      |> Enum.reject(&is_nil(&1.id))
256     -|      |> Enum.uniq_by(& &1.id)
257     -|      |> Enum.sort_by(&(-1 * (&1.score || 0.0)))
258     -|      |> Enum.filter(fn c ->
259     -|        w_score <= 0.0 or c.score >= w_score * (1.0 - margin_window)
260     -|      end)
261     -|      |> Enum.take(top_k)
262     -|      |> Enum.with_index(1)
263     -|      |> Enum.map(fn {c, i} -> %{c | rank: i} end)
    246 +|      # Include the winner explicitly (first, de-duplicated)
    247 +|      winner_as_candidate = %{
    248 +|        id: winner[:id],
    249 +|        score: as_float(w_score),
    250 +|        rank: 0,
    251 +|        from: :atl_winner,
    252 +|        pos: pos_from_id(winner[:id]),
    253 +|        norm: winner[:norm],
    254 +|        lemma: winner[:lemma],
    255 +|        features: Map.get(winner, :raw, %{}) |> Map.get(:features, %{}),
    256 +|        margin: Map.get(winner, :margin, 0.0)
    257 +|      }
264 258  |
265     -|    # Final list for this token
266     -|    {idx, uniq_by_id([winner_as_candidate | near])}
267     -|  end)
268     -|end
    259 +|      # Filter near-winners by margin window around the winner score, take top_k
    260 +|      near =
    261 +|        alts
    262 +|        |> Enum.reject(&is_nil(&1.id))
    263 +|        |> Enum.uniq_by(& &1.id)
    264 +|        |> Enum.sort_by(&(-1 * (&1.score || 0.0)))
    265 +|        |> Enum.filter(fn c ->
    266 +|          w_score <= 0.0 or c.score >= w_score * (1.0 - margin_window)
    267 +|        end)
    268 +|        |> Enum.take(top_k)
    269 +|        |> Enum.with_index(1)
    270 +|        |> Enum.map(fn {c, i} -> %{c | rank: i} end)
269 271  |
270     -|defp uniq_by_id(list),
271     -|  do: list |> Enum.reject(&is_nil(&1.id)) |> Enum.uniq_by(& &1.id)
272     -|
273     -|defp as_float(nil),  do: 0.0
274     -|defp as_float(num) when is_number(num), do: num
275     -|defp as_float(str) when is_binary(str) do
276     -|  case Float.parse(str) do
277     -|    {f, _} -> f
278     -|    _ -> 0.0
    272 +|      # Final list for this token
    273 +|      {idx, uniq_by_id([winner_as_candidate | near])}
    274 +|    end)
279 275  |  end
280     -|end
281 276  |
282     -|defp pos_from_id(nil), do: nil
283     -|defp pos_from_id(id) when is_binary(id) do
284     -|  case String.split(id, "|") do
285     -|    [_lemma, pos, _sense] -> pos
286     -|    _ -> nil
    277 +|  defp uniq_by_id(list),
    278 +|    do: list |> Enum.reject(&is_nil(&1.id)) |> Enum.uniq_by(& &1.id)
    279 +|
    280 +|  defp as_float(nil), do: 0.0
    281 +|  defp as_float(num) when is_number(num), do: num
    282 +|
    283 +|  defp as_float(str) when is_binary(str) do
    284 +|    case Float.parse(str) do
    285 +|      {f, _} -> f
    286 +|      _ -> 0.0
    287 +|    end
287 288  |  end
288     -|end
289 289  |
    290 +|  defp pos_from_id(nil), do: nil
290 291  |
    292 +|  defp pos_from_id(id) when is_binary(id) do
    293 +|    case String.split(id, "|") do
    294 +|      [_lemma, pos, _sense] -> pos
    295 +|      _ -> nil
    296 +|    end
    297 +|  end
291 298  |end
292 299  |
293     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/db/lib/db/episodes.ex
[0m
         |
 32  32  |    vec_limit: 100,
 33  33  |    top_k: 10,
 34     -|    half_life: 86_400, # seconds
 35     -|    alpha: 0.7,        # jaccard weight
 36     -|    beta: 0.2,         # vector weight
 37     -|    gamma: 0.1         # recency weight
     34 +|    # seconds
     35 +|    half_life: 86_400,
     36 +|    # jaccard weight
     37 +|    alpha: 0.7,
     38 +|    # vector weight
     39 +|    beta: 0.2,
     40 +|    # recency weight
     41 +|    gamma: 0.1
 38  42  |  ]
 39  43  |
         |
104 108  |
105 109  |    token_limit = opts[:token_limit]
106     -|    vec_limit   = opts[:vec_limit]
107     -|    top_k       = opts[:top_k]
108     -|    half_life   = opts[:half_life]
109     -|    alpha       = opts[:alpha]
110     -|    beta        = opts[:beta]
111     -|    gamma       = opts[:gamma]
    110 +|    vec_limit = opts[:vec_limit]
    111 +|    top_k = opts[:top_k]
    112 +|    half_life = opts[:half_life]
    113 +|    alpha = opts[:alpha]
    114 +|    beta = opts[:beta]
    115 +|    gamma = opts[:gamma]
112 116  |
113 117  |    now = NaiveDateTime.utc_now()
         |
148 152  |
149 153  |    token_list = Enum.map(token_candidates, &{&1.id, &1})
150     -|    vec_list   = vec_sim_map |> Map.values() |> Enum.map(fn {ep, _sim} -> {ep.id, ep} end)
    154 +|    vec_list = vec_sim_map |> Map.values() |> Enum.map(fn {ep, _sim} -> {ep.id, ep} end)
151 155  |
152 156  |    all_cands =
         |
159 163  |    |> Enum.map(fn {_id, ep} ->
160 164  |      ep_tokens_set = MapSet.new((ep.tokens || []) |> Enum.map(&String.downcase/1))
161     -|      cue_set       = MapSet.new(cues)
    165 +|      cue_set = MapSet.new(cues)
162 166  |
163     -|      inter   = MapSet.intersection(ep_tokens_set, cue_set) |> MapSet.size()
164     -|      union   = MapSet.union(ep_tokens_set, cue_set) |> MapSet.size()
    167 +|      inter = MapSet.intersection(ep_tokens_set, cue_set) |> MapSet.size()
    168 +|      union = MapSet.union(ep_tokens_set, cue_set) |> MapSet.size()
165 169  |      jaccard = if union == 0, do: 0.0, else: inter / union
166 170  |
         |
178 182  |
179 183  |      recency = :math.pow(2.0, -age_s / max(1, half_life))
180     -|      score   = alpha * jaccard + beta * (vec_sim || 0.0) + gamma * recency
    184 +|      score = alpha * jaccard + beta * (vec_sim || 0.0) + gamma * recency
181 185  |
182 186  |      %{
         |
211 215  |  Returns compact maps for pMTG evidence.
212 216  |  """
213     -|# lib/db/episodes.ex
214     -|@spec find_by_lemma(String.t(), non_neg_integer()) :: [map()]
215     -|def find_by_lemma(lemma, limit \\ 5)
216     -|def find_by_lemma(lemma, limit) when is_binary(lemma) and is_integer(limit) and limit > 0 do
217     -|  if String.contains?(lemma, " ") do
218     -|    search(lemma) |> Enum.take(limit)
219     -|  else
220     -|    cues = [String.downcase(lemma)]
221     -|    recall_hybrid(cues, nil, top_k: limit)
222     -|    |> Enum.map(&to_hit/1)
    217 +|  # lib/db/episodes.ex
    218 +|  @spec find_by_lemma(String.t(), non_neg_integer()) :: [map()]
    219 +|  def find_by_lemma(lemma, limit \\ 5)
    220 +|
    221 +|  def find_by_lemma(lemma, limit) when is_binary(lemma) and is_integer(limit) and limit > 0 do
    222 +|    if String.contains?(lemma, " ") do
    223 +|      search(lemma) |> Enum.take(limit)
    224 +|    else
    225 +|      cues = [String.downcase(lemma)]
    226 +|
    227 +|      recall_hybrid(cues, nil, top_k: limit)
    228 +|      |> Enum.map(&to_hit/1)
    229 +|    end
223 230  |  end
224     -|end
225 231  |
226 232  |  @doc """
         |
343 349  |end
344 350  |
345     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/lib/core/token.ex
[0m
         |
159 159  |    `{:error, failures}` where failures is a list of maps describing the bad tokens
160 160  |  """
161     -|# in Core.Token
162 161  |
163     -|@spec check_span_invariants(Core.SemanticInput.t()) ::
164     -|        {:ok, Core.SemanticInput.t()} | {:error, [map()]}
165     -|def check_span_invariants(%Core.SemanticInput{sentence: s, tokens: tokens} = si) do
166     -|  {failures, _last_start} =
167     -|    tokens
168     -|    |> Enum.with_index()
169     -|    |> Enum.reduce({[], -1}, fn {t, idx}, {acc, last_start} ->
170     -|      # tolerate struct or map; atom or string keys
171     -|      p =
172     -|        Map.get(t, :phrase) ||
173     -|          Map.get(t, "phrase") ||
174     -|          ""
    162 +|  # in Core.Token
175 163  |
176     -|      span_raw =
177     -|        Map.get(t, :span) ||
178     -|          Map.get(t, "span") ||
179     -|          {0, 0}
    164 +|  @spec check_span_invariants(Core.SemanticInput.t()) ::
    165 +|          {:ok, Core.SemanticInput.t()} | {:error, [map()]}
    166 +|  def check_span_invariants(%Core.SemanticInput{sentence: s, tokens: tokens} = si) do
    167 +|    {failures, _last_start} =
    168 +|      tokens
    169 +|      |> Enum.with_index()
    170 +|      |> Enum.reduce({[], -1}, fn {t, idx}, {acc, last_start} ->
    171 +|        # tolerate struct or map; atom or string keys
    172 +|        p =
    173 +|          Map.get(t, :phrase) ||
    174 +|            Map.get(t, "phrase") ||
    175 +|            ""
180 176  |
181     -|      {st, en} = normalize_span(span_raw)
182     -|      slice = safe_slice(s, st, en)
    177 +|        span_raw =
    178 +|          Map.get(t, :span) ||
    179 +|            Map.get(t, "span") ||
    180 +|            {0, 0}
183 181  |
184     -|      reasons = []
185     -|      reasons = if is_binary(p), do: reasons, else: [{:phrase_type, p} | reasons]
186     -|      reasons = if en >= st, do: reasons, else: [{:order, {st, en}} | reasons]
187     -|      reasons = if p == slice, do: reasons, else: [{:slice_mismatch, {p, slice}} | reasons]
188     -|      reasons = if st >= last_start, do: reasons, else: [{:start_order, {last_start, st}} | reasons]
    182 +|        {st, en} = normalize_span(span_raw)
    183 +|        slice = safe_slice(s, st, en)
189 184  |
190     -|      if reasons == [] do
191     -|        {acc, st}
192     -|      else
193     -|        fail = %{
194     -|          index: idx,
195     -|          token: t,
196     -|          span: {st, en},
197     -|          expected_slice: slice,
198     -|          reasons: Enum.reverse(reasons)
199     -|        }
    185 +|        reasons = []
    186 +|        reasons = if is_binary(p), do: reasons, else: [{:phrase_type, p} | reasons]
    187 +|        reasons = if en >= st, do: reasons, else: [{:order, {st, en}} | reasons]
    188 +|        reasons = if p == slice, do: reasons, else: [{:slice_mismatch, {p, slice}} | reasons]
200 189  |
201     -|        {[fail | acc], st}
202     -|      end
203     -|    end)
    190 +|        reasons =
    191 +|          if st >= last_start, do: reasons, else: [{:start_order, {last_start, st}} | reasons]
204 192  |
205     -|  case failures do
206     -|    [] -> {:ok, si}
207     -|    _ -> {:error, Enum.reverse(failures)}
208     -|  end
209     -|end
    193 +|        if reasons == [] do
    194 +|          {acc, st}
    195 +|        else
    196 +|          fail = %{
    197 +|            index: idx,
    198 +|            token: t,
    199 +|            span: {st, en},
    200 +|            expected_slice: slice,
    201 +|            reasons: Enum.reverse(reasons)
    202 +|          }
210 203  |
211     -|# Accept tuple or [start,end]; default safely
212     -|defp normalize_span({st, en}) when is_integer(st) and is_integer(en), do: {st, en}
213     -|defp normalize_span([st, en]) when is_integer(st) and is_integer(en), do: {st, en}
214     -|defp normalize_span(_), do: {0, 0}
    204 +|          {[fail | acc], st}
    205 +|        end
    206 +|      end)
215 207  |
216     -|defp safe_slice(s, st, en)
217     -|     when is_binary(s) and is_integer(st) and is_integer(en) and en >= st do
218     -|  String.slice(s, st, en - st) || ""
219     -|end
    208 +|    case failures do
    209 +|      [] -> {:ok, si}
    210 +|      _ -> {:error, Enum.reverse(failures)}
    211 +|    end
    212 +|  end
220 213  |
221     -|defp safe_slice(_s, _st, _en), do: ""
    214 +|  # Accept tuple or [start,end]; default safely
    215 +|  defp normalize_span({st, en}) when is_integer(st) and is_integer(en), do: {st, en}
    216 +|  defp normalize_span([st, en]) when is_integer(st) and is_integer(en), do: {st, en}
    217 +|  defp normalize_span(_), do: {0, 0}
222 218  |
    219 +|  defp safe_slice(s, st, en)
    220 +|       when is_binary(s) and is_integer(st) and is_integer(en) and en >= st do
    221 +|    String.slice(s, st, en - st) || ""
    222 +|  end
    223 +|
    224 +|  defp safe_slice(_s, _st, _en), do: ""
223 225  |end
224 226  |
225     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/test/core/lifg_input_test.exs
[0m
       |
20 20  |    assert Enum.map(tokens, &{&1.phrase, &1.span, &1.n}) == [
21 21  |             {"hello there", {0, 11}, 2},
22    -|             {"hello",       {0, 5},  1},
23    -|             {"there",       {6, 11}, 1}
   22 +|             {"hello", {0, 5}, 1},
   23 +|             {"there", {6, 11}, 1}
24 24  |           ]
25 25  |
       |
34 34  |  """
35 35  |  test "SI path: returns SI with boundary-aligned char spans and invariant OK" do
36    -|    si_in  = %SemanticInput{sentence: "hello there"}
   36 +|    si_in = %SemanticInput{sentence: "hello there"}
37 37  |    si_out = Input.tokenize(si_in)
38 38  |
       |
43 43  |    assert Enum.map(si_out.tokens, &{&1.phrase, &1.span}) == [
44 44  |             {"hello there", {0, 11}},
45    -|             {"hello",       {0, 5}},
46    -|             {"there",       {6, 11}}
   45 +|             {"hello", {0, 5}},
   46 +|             {"there", {6, 11}}
47 47  |           ]
48 48  |  end
49 49  |end
50 50  |
51    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/test/brain/pmtg_rerun_test.exs
[0m
       |
81 81  |end
82 82  |
83    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/test/brain/bench/bench_brain_lifg_bench.exs
[0m
       |
12 12  |defmodule BenchUtil do
13 13  |  def randf, do: :rand.uniform()
14    -|  def vec(dim), do: for _ <- 1..dim, do: :rand.uniform() - 0.5
   14 +|  def vec(dim), do: for(_ <- 1..dim, do: :rand.uniform() - 0.5)
15 15  |
16 16  |  # Build a full SI (current API): tokens + sense_candidates + context_vec
       |
28 28  |            %{
29 29  |              id: "t#{t}|s#{s}",
30    -|              prior: randf(), # feeds :rel_prior through candidates_from_slate/… → score mix
   30 +|              # feeds :rel_prior through candidates_from_slate/… → score mix
   31 +|              prior: randf(),
31 32  |              features: %{
32 33  |                pos: "noun",
       |
51 52  |
52 53  |scenarios = [
53    -|  {:g4_s3_d64,   4, 3, 64},
54    -|  {:g8_s3_d64,   8, 3, 64},
55    -|  {:g8_s6_d64,   8, 6, 64},
   54 +|  {:g4_s3_d64, 4, 3, 64},
   55 +|  {:g8_s3_d64, 8, 3, 64},
   56 +|  {:g8_s6_d64, 8, 6, 64},
56 57  |  {:g16_s4_d64, 16, 4, 64},
57    -|  {:g8_s3_d128,  8, 3, 128}
   58 +|  {:g8_s3_d128, 8, 3, 128}
58 59  |]
59 60  |
       |
85 86  |)
86 87  |
87    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/lib/core/lexicon/senses.ex
[0m
       |
27 27  |end
28 28  |
29    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/db/lib/db/lexicon.ex
[0m
         |
100 100  |  @spec bulk_upsert_senses(list()) :: :ok
101 101  |  def bulk_upsert_senses([]), do: :ok
    102 +|
102 103  |  def bulk_upsert_senses(rows) when is_list(rows) do
103 104  |    now = NaiveDateTime.utc_now() |> NaiveDateTime.truncate(:second)
         |
107 108  |        word = get(r, :word, "")
108 109  |        norm = normize(word)
109     -|        pos  = get(r, :pos, "unk")
110     -|        id   = get(r, :id) || build_lex_id(norm, pos, r)
    110 +|        pos = get(r, :pos, "unk")
    111 +|        id = get(r, :id) || build_lex_id(norm, pos, r)
111 112  |
112 113  |        %{
         |
150 151  |  end
151 152  |
152     -|# --- PMTG compatibility: lightweight sense lookups -------------------
    153 +|  # --- PMTG compatibility: lightweight sense lookups -------------------
153 154  |
154     -|@doc """
155     -|Return up to `limit` senses for a lemma, shaped for PMTG evidence.
    155 +|  @doc """
    156 +|  Return up to `limit` senses for a lemma, shaped for PMTG evidence.
156 157  |
157     -|Reads from BrainCell rows where `norm == normize(lemma)`. Prefers rows
158     -|seeded via `bulk_upsert_senses/1` (type = "lexicon") but will also
159     -|return other types if present.
160     -|"""
161     -|@spec lookup(String.t(), non_neg_integer()) :: [map()]
162     -|def lookup(lemma, limit) when is_binary(lemma) and is_integer(limit) do
163     -|  norm = normize(lemma)
    158 +|  Reads from BrainCell rows where `norm == normize(lemma)`. Prefers rows
    159 +|  seeded via `bulk_upsert_senses/1` (type = "lexicon") but will also
    160 +|  return other types if present.
    161 +|  """
    162 +|  @spec lookup(String.t(), non_neg_integer()) :: [map()]
    163 +|  def lookup(lemma, limit) when is_binary(lemma) and is_integer(limit) do
    164 +|    norm = normize(lemma)
164 165  |
165     -|  Db.all(
166     -|    from b in BrainCell,
167     -|      where: b.norm == ^norm and b.status == "active",
168     -|      order_by: [
169     -|        asc: fragment("CASE WHEN ? = 'lexicon' THEN 0 ELSE 1 END", b.type),
170     -|        desc: b.updated_at
171     -|      ],
172     -|      limit: ^max(limit, 0),
173     -|      select: %{
174     -|        id: b.id,
175     -|        lemma: b.word,
176     -|        pos: b.pos,
177     -|        type: b.type,
178     -|        gloss: b.definition,
179     -|        example: b.example,
180     -|        synonyms: b.synonyms,
181     -|        antonyms: b.antonyms
182     -|      }
183     -|  )
184     -|end
    166 +|    Db.all(
    167 +|      from(b in BrainCell,
    168 +|        where: b.norm == ^norm and b.status == "active",
    169 +|        order_by: [
    170 +|          asc: fragment("CASE WHEN ? = 'lexicon' THEN 0 ELSE 1 END", b.type),
    171 +|          desc: b.updated_at
    172 +|        ],
    173 +|        limit: ^max(limit, 0),
    174 +|        select: %{
    175 +|          id: b.id,
    176 +|          lemma: b.word,
    177 +|          pos: b.pos,
    178 +|          type: b.type,
    179 +|          gloss: b.definition,
    180 +|          example: b.example,
    181 +|          synonyms: b.synonyms,
    182 +|          antonyms: b.antonyms
    183 +|        }
    184 +|      )
    185 +|    )
    186 +|  end
185 187  |
186     -|@doc "Alias kept for compatibility with other callers."
187     -|@spec definitions_for(String.t(), non_neg_integer()) :: [map()]
188     -|def definitions_for(lemma, limit), do: lookup(lemma, limit)
189     -|
190     -|
    188 +|  @doc "Alias kept for compatibility with other callers."
    189 +|  @spec definitions_for(String.t(), non_neg_integer()) :: [map()]
    190 +|  def definitions_for(lemma, limit), do: lookup(lemma, limit)
191 191  |end
192 192  |
193     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/test/brain/boundary_guard_test.exs
[0m
       |
11 11  |  test "drops tokens not aligned to word boundaries when sentence is present" do
12 12  |    sent = "hello"
13    -|    toks = [%{index: 0, phrase: "he", span: {1, 2}}] # mid-word slice
   13 +|    # mid-word slice
   14 +|    toks = [%{index: 0, phrase: "he", span: {1, 2}}]
14 15  |    assert [] = BoundaryGuard.sanitize(toks, sent)
15 16  |  end
       |
22 23  |end
23 24  |
24    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/test/brain/reanalysis_test.exs
[0m
       |
 7  7  |      sense_candidates: %{
 8  8  |        0 => [
 9    -|          %{id: "top",    features: %{lex_fit: 0.9, rel_prior: 0.6, activation: 0.1, intent_bias: 0.0}},
10    -|          %{id: "runner", features: %{lex_fit: 0.8, rel_prior: 0.6, activation: 0.1, intent_bias: 0.0}}
    9 +|          %{
   10 +|            id: "top",
   11 +|            features: %{lex_fit: 0.9, rel_prior: 0.6, activation: 0.1, intent_bias: 0.0}
   12 +|          },
   13 +|          %{
   14 +|            id: "runner",
   15 +|            features: %{lex_fit: 0.8, rel_prior: 0.6, activation: 0.1, intent_bias: 0.0}
   16 +|          }
11 17  |        ]
12 18  |      }
       |
20 26  |end
21 27  |
22    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/lib/core/lifg_input.ex
[0m
         |
 29  29  |
 30  30  |    # ★ Build SI with char spans so downstream slices always match
 31     -|    si0 = Token.tokenize(sentence, Keyword.merge(tok_opts, span_mode: :chars))  # ★
     31 +|    # ★
     32 +|    si0 = Token.tokenize(sentence, Keyword.merge(tok_opts, span_mode: :chars))
 32  33  |
 33  34  |    # ★ Invariant check (telemetry on failure, proceed defensively)
 34  35  |    si1 =
 35  36  |      case Token.check_span_invariants(si0) do
 36     -|        {:ok, si} -> si
     37 +|        {:ok, si} ->
     38 +|          si
     39 +|
 37  40  |        {:error, fails} ->
 38  41  |          :telemetry.execute(
 39     -|            [:core, :token, :span_invariant_fail],                 # ★
     42 +|            # ★
     43 +|            [:core, :token, :span_invariant_fail],
 40  44  |            %{count: length(fails)},
 41  45  |            %{fails: fails, sentence: si0.sentence}
 42  46  |          )
     47 +|
 43  48  |          si0
 44  49  |      end
         |
 57  62  |
 58  63  |    # ★ Use the SI's sentence; produce char spans
 59     -|    si0 = Token.tokenize(sentence, Keyword.merge(tok_opts, span_mode: :chars))  # ★
     64 +|    # ★
     65 +|    si0 = Token.tokenize(sentence, Keyword.merge(tok_opts, span_mode: :chars))
 60  66  |
 61  67  |    # ★ Invariant check (telemetry on failure)
 62  68  |    si1 =
 63  69  |      case Token.check_span_invariants(si0) do
 64     -|        {:ok, si_ok} -> si_ok
     70 +|        {:ok, si_ok} ->
     71 +|          si_ok
     72 +|
 65  73  |        {:error, fails} ->
 66  74  |          :telemetry.execute(
 67     -|            [:core, :token, :span_invariant_fail],                 # ★
     75 +|            # ★
     76 +|            [:core, :token, :span_invariant_fail],
 68  77  |            %{count: length(fails)},
 69  78  |            %{fails: fails, sentence: si0.sentence}
 70  79  |          )
     80 +|
 71  81  |          si0
 72  82  |      end
         |
 84  94  |
 85  95  |    # ★ Keep the sentence from si1 (normalized) and replace tokens
 86     -|    %Core.SemanticInput{si1 | tokens: tokens_final}                              # ★
     96 +|    # ★
     97 +|    %Core.SemanticInput{si1 | tokens: tokens_final}
 87  98  |  end
 88  99  |
         |
101 112  |    #    (:mode, :emit_chargrams) remain harmless no-ops for compatibility.
102 113  |    env = Application.get_env(:core, :tokenizer_defaults, [])
103     -|    [max_wordgram_n: 3, emit_chargrams: false] |> Keyword.merge(env)             # ★
    114 +|    # ★
    115 +|    [max_wordgram_n: 3, emit_chargrams: false] |> Keyword.merge(env)
104 116  |  end
105 117  |end
106 118  |
107     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/lib/brain/telemetry.ex
[0m
       |
82 82  |end
83 83  |
84    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/lexicon/mix.exs
[0m
       |
34 34  |end
35 35  |
36    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/db/lib/db/episode.ex
[0m
         |
 12  12  |  • `si` is normalized to a plain map, recursively converting structs (e.g., `Core.SemanticInput`, `%Db.BrainCell{}`) and data structures (e.g., `MapSet` to list) for JSONB serialization.
 13  13  |  """
 14     -|                                                                 
     14 +|
 15  15  |  use Ecto.Schema
 16  16  |  import Ecto.Changeset
 17  17  |  require Logger
 18  18  |
 19     -|  @type t :: %__MODULE__{}                                            
 20     -|  @embedding_dim Application.compile_env(:db, :embedding_dim, 1536)   
     19 +|  @type t :: %__MODULE__{}
     20 +|  @embedding_dim Application.compile_env(:db, :embedding_dim, 1536)
 21  21  |
 22     -|  schema "episodes" do                                                   
 23     -|    field :user_id, :binary_id
 24     -|    field :tokens, {:array, :string}, default: []                         
 25     -|    field :token_count, :integer, default: 0
 26     -|    field :si, :map, default: %{}
 27     -|    field :embedding, Pgvector.Ecto.Vector
 28     -|    field :tags, {:array, :string}, default: []
     22 +|  schema "episodes" do
     23 +|    field(:user_id, :binary_id)
     24 +|    field(:tokens, {:array, :string}, default: [])
     25 +|    field(:token_count, :integer, default: 0)
     26 +|    field(:si, :map, default: %{})
     27 +|    field(:embedding, Pgvector.Ecto.Vector)
     28 +|    field(:tags, {:array, :string}, default: [])
 29  29  |    timestamps(type: :naive_datetime_usec)
 30  30  |  end
         |
 54  54  |  defp normalize_tokens(changeset) do
 55  55  |    case get_change(changeset, :tokens) do
 56     -|      nil -> changeset
     56 +|      nil ->
     57 +|        changeset
     58 +|
 57  59  |      list when is_list(list) ->
 58  60  |        put_change(changeset, :tokens, Enum.map(list, &to_string/1))
     61 +|
 59  62  |      other ->
 60  63  |        add_error(changeset, :tokens, "must be a list of strings, got: #{inspect(other)}")
         |
 65  68  |  defp normalize_si(changeset) do
 66  69  |    case get_change(changeset, :si) do
 67     -|      nil -> changeset
     70 +|      nil ->
     71 +|        changeset
     72 +|
 68  73  |      %{} = si ->
 69  74  |        normalized = to_plain_map(si)
 70     -|        Logger.debug("Si normalized: #{inspect(Map.keys(normalized), limit: 10)}")  # Debug: Check top-level keys
     75 +|        # Debug: Check top-level keys
     76 +|        Logger.debug("Si normalized: #{inspect(Map.keys(normalized), limit: 10)}")
 71  77  |        put_change(changeset, :si, normalized)
     78 +|
 72  79  |      other ->
 73  80  |        add_error(changeset, :si, "must be a map, got: #{inspect(other)}")
         |
 80  87  |    struct
 81  88  |    |> Map.from_struct()
 82     -|    |> Map.delete(:__meta__)  # Drop Ecto metadata
     89 +|    # Drop Ecto metadata
     90 +|    |> Map.delete(:__meta__)
 83  91  |    |> Map.new(fn {k, v} ->
 84  92  |      {k, to_plain_map(v)}
         |
117 125  |    |> Map.update(:activation_summary, %{}, fn summary ->
118 126  |      Map.update(summary, :db_hits, [], fn hits ->
119     -|        to_plain_map(hits)  # Ensures MapSet → list
    127 +|        # Ensures MapSet → list
    128 +|        to_plain_map(hits)
120 129  |      end)
121 130  |    end)
122 131  |    |> Map.update(:trace, [], fn trace ->
123 132  |      Enum.map(trace, fn item ->
124     -|        to_plain_map(item)  # Handles tuples → lists or maps
    133 +|        # Handles tuples → lists or maps
    134 +|        to_plain_map(item)
125 135  |      end)
126 136  |    end)
127 137  |    |> Map.update(:active_cells, [], fn cells ->
128     -|      Enum.map(cells, &to_plain_map/1)  # Ensures BrainCell structs → plain maps
    138 +|      # Ensures BrainCell structs → plain maps
    139 +|      Enum.map(cells, &to_plain_map/1)
129 140  |    end)
130 141  |  end
         |
141 152  |
142 153  |    cond do
143     -|      is_nil(val) -> changeset
    154 +|      is_nil(val) ->
    155 +|        changeset
    156 +|
144 157  |      match?(%Pgvector{}, val) ->
145 158  |        case Pgvector.to_list(val) do
146     -|          list when is_list(list) and length(list) == @embedding_dim -> changeset
147     -|          list when is_list(list) -> add_error(changeset, :embedding, "embedding length must be #{@embedding_dim} (got #{length(list)})")
148     -|          _ -> add_error(changeset, :embedding, "invalid pgvector value")
    159 +|          list when is_list(list) and length(list) == @embedding_dim ->
    160 +|            changeset
    161 +|
    162 +|          list when is_list(list) ->
    163 +|            add_error(
    164 +|              changeset,
    165 +|              :embedding,
    166 +|              "embedding length must be #{@embedding_dim} (got #{length(list)})"
    167 +|            )
    168 +|
    169 +|          _ ->
    170 +|            add_error(changeset, :embedding, "invalid pgvector value")
149 171  |        end
    172 +|
150 173  |      is_list(val) ->
151 174  |        if length(val) == @embedding_dim do
152 175  |          changeset
153 176  |        else
154     -|          add_error(changeset, :embedding, "embedding length must be #{@embedding_dim} (got #{length(val)})")
    177 +|          add_error(
    178 +|            changeset,
    179 +|            :embedding,
    180 +|            "embedding length must be #{@embedding_dim} (got #{length(val)})"
    181 +|          )
155 182  |        end
    183 +|
156 184  |      true ->
157 185  |        add_error(changeset, :embedding, "embedding must be a list of floats or a %Pgvector{}")
         |
160 188  |end
161 189  |
162     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/lib/core/lexicon.ex
[0m
         |
 13  13  |  alias Core.Lexicon.Stage
 14  14  |  alias Db.Lexicon, as: DbLex
 15     -|alias Db
 16     -|#alias Db.BrainCell
     15 +|  alias Db
     16 +|  # alias Db.BrainCell
 17  17  |
 18  18  |  @doc """
         |
 26  26  |      norms =
 27  27  |        si1.tokens
 28     -|        |> Enum.map(&(&1[:phrase]))
     28 +|        |> Enum.map(& &1[:phrase])
 29  29  |        |> Enum.filter(&is_binary/1)
 30  30  |        |> Enum.map(&String.downcase/1)
         |
 58  58  |  end
 59  59  |
 60     -|@doc """
 61     -|Ensure a seed BrainCell exists for each missing norm.
 62     -|Creates rows like "\#{norm}|unk|seed|" with type="seed" and pos="unk".
 63     -|No-ops on conflict.
 64     -|"""
 65     -|@spec ensure_cells([String.t()]) :: :ok
 66     -|def ensure_cells(norms) when is_list(norms) do
 67     -|  norms =
 68     -|    norms
 69     -|    |> Enum.filter(&is_binary/1)
 70     -|    |> Enum.map(&String.downcase/1)
 71     -|    |> Enum.uniq()
 72     -|    |> Enum.reject(&String.contains?(&1, " "))   # ← skip multi-word expressions
     60 +|  @doc """
     61 +|  Ensure a seed BrainCell exists for each missing norm.
     62 +|  Creates rows like "\#{norm}|unk|seed|" with type="seed" and pos="unk".
     63 +|  No-ops on conflict.
     64 +|  """
     65 +|  @spec ensure_cells([String.t()]) :: :ok
     66 +|  def ensure_cells(norms) when is_list(norms) do
     67 +|    norms =
     68 +|      norms
     69 +|      |> Enum.filter(&is_binary/1)
     70 +|      |> Enum.map(&String.downcase/1)
     71 +|      |> Enum.uniq()
     72 +|      # ← skip multi-word expressions
     73 +|      |> Enum.reject(&String.contains?(&1, " "))
 73  74  |
 74     -|  if norms == [] do
 75     -|    :ok
 76     -|  else
 77     -|    now = NaiveDateTime.utc_now() |> NaiveDateTime.truncate(:second)
     75 +|    if norms == [] do
     76 +|      :ok
     77 +|    else
     78 +|      now = NaiveDateTime.utc_now() |> NaiveDateTime.truncate(:second)
 78  79  |
 79     -|    rows =
 80     -|      for norm <- norms do
 81     -|        %{
 82     -|          id: "#{norm}|unk|seed|",
 83     -|          status: "active",
 84     -|          type: "seed",
 85     -|          norm: norm,
 86     -|          pos: "unk",
 87     -|          word: norm,
 88     -|          inserted_at: now,
 89     -|          updated_at: now
 90     -|        }
 91     -|      end
     80 +|      rows =
     81 +|        for norm <- norms do
     82 +|          %{
     83 +|            id: "#{norm}|unk|seed|",
     84 +|            status: "active",
     85 +|            type: "seed",
     86 +|            norm: norm,
     87 +|            pos: "unk",
     88 +|            word: norm,
     89 +|            inserted_at: now,
     90 +|            updated_at: now
     91 +|          }
     92 +|        end
 92  93  |
 93     -|    _ = Db.insert_all(Db.BrainCell, rows, on_conflict: :nothing)
 94     -|    :ok
     94 +|      _ = Db.insert_all(Db.BrainCell, rows, on_conflict: :nothing)
     95 +|      :ok
     96 +|    end
 95  97  |  end
 96     -|end
 97  98  |
 98     -|
 99     -|
100  99  |  @doc """
101 100  |  DB bulk upsert for senses (called by Stage).
         |
116 115  |      Enum.reduce(list, {[], MapSet.new()}, fn x, {acc, seen} ->
117 116  |        case get_id(x) do
118     -|          nil -> {acc, seen}
    117 +|          nil ->
    118 +|            {acc, seen}
    119 +|
119 120  |          id ->
120 121  |            if MapSet.member?(seen, id) do
         |
130 131  |end
131 132  |
132     -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/core/test/test_helper.exs
[0m
       |
 9  9  |Ecto.Adapters.SQL.Sandbox.mode(Db, :manual)
10 10  |
11    -|

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/symbrella/lib/symbrella/application.ex
[0m
       |
26 26  |      # If you later want the Endpoint here, add it here and remove from web.
27 27  |      #
28    -|Brain.LIFG,
29    -|Brain.PMTG,
30    -|{Brain.ATL, keep: 300}
31    -|      #{Brain.Hippocampus, keep: 300},
32    -|  #Brain.AG,
33    -|  #Brain.MTL,
34    -|  #Brain.ACC,
35    -|  #Brain.BasalGanglia
   28 +|      Brain.LIFG,
   29 +|      Brain.PMTG,
   30 +|      {Brain.ATL, keep: 300}
   31 +|      # {Brain.Hippocampus, keep: 300},
   32 +|      # Brain.AG,
   33 +|      # Brain.MTL,
   34 +|      # Brain.ACC,
   35 +|      # Brain.BasalGanglia
36 36  |    ]
37 37  |
       |

[1m[31m/data/data/com.termux/files/home/Symbrella/apps/brain/test/brain/lifg_guard_test.exs
[0m
       |
11 11  |
12 12  |  test "sorts by span if all spans present" do
13    -|    toks = [%{phrase: "b", span: {2,1}, index: 1}, %{phrase: "a", span: {0,1}, index: 0}]
   13 +|    toks = [%{phrase: "b", span: {2, 1}, index: 1}, %{phrase: "a", span: {0, 1}, index: 0}]
14 14  |    out = Guard.sanitize(toks)
15 15  |    assert Enum.map(out, & &1.phrase) == ["a", "b"]
       |
17 17  |end
18 18  |
19    -|



==> lexicon
Compiling 2 files (.ex)
Generated lexicon app
==> db
Compiling 7 files (.ex)
Compiling lib/db/postgrex_types.ex (it's taking more than 10s)
Generated db app
==> brain
Compiling 6 files (.ex)
     warning: variable "slate" is unused (if the variable is not meant to be used, prefix it with an underscore)
     │
 204 │ def promote_sense_candidates_from_slate(%{winners: winners} = slate, opts) do
     │                                                               ~~~~~
     │
     └─ (brain 0.1.0) lib/brain/atl.ex:204:63: Brain.ATL.promote_sense_candidates_from_slate/2

     warning: the default value for the last optional argument in safe_exec_telemetry/3 is never used
     │
 566 │   defp safe_exec_telemetry(event, measurements \\ %{}, meta \\ %{}) do
     │        ~
     │
     └─ lib/brain/ptmg.ex:566:8: Brain.PMTG (module)

     warning: default values for the optional arguments in emit_rerun_event/2 are never used
     │
 556 │   defp emit_rerun_event(choices, mode \\ :sync) do
     │        ~
     │
     └─ lib/brain/ptmg.ex:556:8: Brain.PMTG (module)

     warning: def low_confidence?/2 has multiple clauses and also declares default values. In such cases, the default values should be defined in a header. Instead of:

         def foo(:first_clause, b \\ :default) do ... end
         def foo(:second_clause, b) do ... end

     one should write:

         def foo(a, b \\ :default)
         def foo(:first_clause, b) do ... end
         def foo(:second_clause, b) do ... end

     │
 518 │   def low_confidence?(_choice, _opts), do: true
     │       ~
     │
     └─ lib/brain/lifg.ex:518:7

     warning: clauses with the same name and arity (number of arguments) should be grouped together, "defp letter?/1" was previously defined (lib/brain/lifg.ex:898)
     │
 902 │     defp letter?(_), do: false
     │          ~
     │
     └─ lib/brain/lifg.ex:902:10

      warning: function pick_winner_with_margin/1 is unused
      │
 1109 │ defp pick_winner_with_margin(scored) when is_list(scored) and scored != [] do
      │      ~
      │
      └─ lib/brain/lifg.ex:1109:6: Brain.LIFG.Stage1 (module)

      warning: function margin_threshold/0 is unused
      │
 1104 │ defp margin_threshold do
      │      ~
      │
      └─ lib/brain/lifg.ex:1104:6: Brain.LIFG.Stage1 (module)

      warning: function emit_sense_candidates/4 is unused
      │
 1116 │ defp emit_sense_candidates(si, token_index, scored, lemma) do
      │      ~
      │
      └─ lib/brain/lifg.ex:1116:6: Brain.LIFG.Stage1 (module)

     warning: module attribute @small_k_cutoff was set but never used
     │
 773 │     @small_k_cutoff 4
     │     ~~~~~~~~~~~~~~~~~
     │
     └─ lib/brain/lifg.ex:773: Brain.LIFG.Stage1 (module)

     warning: function lex_fit_from_phrase/2 is unused
     │
 205 │   defp lex_fit_from_phrase(phrase, lemma) do
     │        ~
     │
     └─ lib/brain/lifg.ex:205:8: Brain.LIFG (module)

     warning: this clause of defp word_char?/1 is never used
     │
 751 │     defp word_char?(nil), do: false
     │          ~
     │
     └─ (brain 0.1.0) lib/brain/lifg.ex:751:10: Brain.LIFG.BoundaryGuard.word_char?/1

      warning: Brain.ATL.finalize/2 is undefined or private
      │
 1184 │           case Brain.ATL.finalize(si, opts) do
      │                          ~
      │
      └─ (brain 0.1.0) lib/brain/lifg.ex:1184:26: Brain.LIFG.run/2

Compilation failed due to warnings while using the --warnings-as-errors option
