# Symbrella Devlog

## 2025-12-10 — LIFG + PMTG start to feel “alive”

Today was the first time LIFG + PMTG actually *felt* like a living piece of the system instead of just a pile of modules and tests.

### What changed (tech)

- Refined `Brain.LIFG.disambiguate_stage1/2` to:
  - Normalize everything through `SemanticInput` and `sense_candidates`.
  - Enforce boundary guards + no-char-gram invariants along the way.
  - Emit a clean trace event with `choices`, `boosts`, and `inhibitions` for downstream use.

- Gave `Brain.PMTG` a proper shape:
  - Added `resolve/3` and `fetch_evidence/3` so pMTG can plan queries and gather episodes/lexicon hits in a structured way.
  - Enforced MWE vs unigram sense compatibility so multiword tokens don’t compete with unigram senses that don’t fit.
  - Added telemetry for `[:brain, :pmtg, :no_mwe_senses]` to catch cases where MWEs get no compatible senses.
  - Wired in a `:boost` vs `:rerun` mode so pMTG can either nudge activations or fully rerun Stage1 with a fresh slate.

- Cleaned up LIFG tests:
  - `BrainLIFGTest`, `BrainLIFGIntegrationTest`, `Brain.LIFGAlignmentTest`, and property tests around boosts/inhibitions and softmax normalization are all green again.
  - Integration test now exercises the feedback loop: winners get boosted, losers get inhibited, and margins stabilize over multiple rounds.

### What I saw (behavior)

I tried a simple input:

> “how are you?”

The Stage1 + MWE pipeline produced something surprisingly “right”:

- Recognized MWEs like `"how are you?"`, `"how are"`, `"are you?"`, plus unigrams `how`, `are`, `you?`.
- Chose reasonable phrase-level fallbacks and lexical senses.
- Most striking: the token for `you` ended up mapped to an assistant-flavored sense (`"symbrella|assistant|0"`), which actually matched the way the system should understand that pronoun in this context.

It’s a small thing, but it’s the first moment where the combination of:
- LIFG competitive sense selection,
- MWEs,
- PMTG guardrails,
- and the trace / audit

all worked together in a way that *felt* more like a brain and less like a bag of tricks.

### How it felt (journey, not just destination)

This didn’t come from a single clever idea. It came from:

- Breaking things with refactors, then forcing tests to tell the truth.
- Chasing down char-gram leaks, boundary guards, and MWE edge cases.
- Iterating on PMTG until it stopped being a vague concept and became an actual “controlled semantic retrieval” region with clear responsibilities.

It mirrors a bigger pattern in my life right now: the work is in **showing up**, again and again, even when it’s frustrating or exhausting. Symbrella evolving from “rough idea” to “coherent brain-ish system” is the same energy as recovery work: it’s not about magically arriving at a perfect endpoint, it’s about staying in the process.

Today’s win: LIFG + PMTG are no longer just theory and tests — they’re starting to behave like something I can talk *with*, not just talk *about*.

### Next directions

- Tighten PMTG ↔ LIFG integration:
  - Make pMTG reruns a standard path whenever decisions are weak or conflict is high.
  - Surface episodic hits explicitly in the trace so I can see *why* a rerun flipped a winner.

- Bring Hippocampus more into the loop:
  - Use episodes as real evidence for PMTG (with recency and conflict-aware weighting).
  - Log when episodic evidence actually changes a LIFG decision.

- Continue hardening invariants:
  - Keep char-grams out of the LIFG path.
  - Ensure MWEs and unigrams stay in their own lanes in both lexicon and episodes.
  - Maintain alignment tests so every choice stays glued to its `token_index`.

The destination will keep moving. The important part is that the journey is starting to feel coherent — in the code and in my own head.

